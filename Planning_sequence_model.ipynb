{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd5d68a-4289-41f7-838e-495ecf0d995d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Simulating Oliver's consolidation task\n",
    "\n",
    "Consolidation can be thought of as training a neocortical generative / predictive model on replayed hippocampal memories. For sequential data, the generative model could correspond to an autoregressive sequence model like GPT-2 that learns to predict the next item in the sequence (by minimising the prediction error on sequences from the training data). The stimuli for Oliver's task can be represented as sequences of form 'START: yellow fruit, STOP: red, REWARD: animal, SEQUENCE: yellow fruit (-1), green animal (2), red animal (2)', which makes it straightforward to train GPT-2.\n",
    "\n",
    "This notebook simulates the task as follows:\n",
    "* Pre-train model so that it learns the *rules* of task\n",
    "* Train on stimuli for task, representing consolidation\n",
    "* Compare generative model accept / reject performance before and after consolidation\n",
    "\n",
    "#### Details\n",
    "\n",
    "Pre-train model on arbitrary stimuli, to model learning the rules of the task. After this stage of training, the model can be given a prompt with randomly chosen stimuli / conditions, e.g. 'START: blue chair, STOP: sad, REWARD: bug, SEQUENCE:', and generate a sequence consistent with the rules, e.g. 'blue chair (-1), sad bug (2)'. But the model knows nothing about the task stimuli or their order. This is supposed to be equivalent to the participant at the point they pass the rules quiz, prior to experiencing the stimuli.\n",
    "\n",
    "I ran the simulation for reward and transition revaluation separately (so there was enough data in each case - I could do both at once though, there'd just be less data to train on):\n",
    "* Reward revaluation: train on 2/3 reward categories in each trial, test on held-out reward category. Repeat n times, each time with a random stimuli order and a random reward condition.\n",
    "* Transition revaluation: train on 2/3 stop colours in each trial, test on held-out transition category. Repeat n times, each time with a random stimuli order and a random stop condition.\n",
    "\n",
    "Note that this is more training data than in Oliver's task, as discussed.\n",
    "\n",
    "The accept / reject task is modelled as follows:\n",
    "* Get the predicted sequence from the model given an input of form 'START: yellow animal, STOP: red, REWARD: animal, SEQUENCE:'\n",
    "* Extract the predicted rewards (any numbers in round brackets)\n",
    "* If predicted reward sum > 0, accept, and if predicted reward sum <= 0, reject\n",
    "* Calculate the accuracy of the accept / reject decisions by comparing with the real sequences\n",
    "* There are 81/3 = 27 test sequences in each case. Of these 9 are of length 1 (with the stop condition met by the start stimulus). I excluded these as these don't require any learning of the sequence to predict the reward.\n",
    "* As described above, there were multiple trials for each of the reward and transition revaluation tasks.\n",
    "\n",
    "#### Results\n",
    "\n",
    "The result is that the generative model (pre-trained on the rules of the task) learns the sequence of stimuli through consolidation, so that it can do reward and transition revaluation fairly well. \n",
    "\n",
    "I haven't modelled the hippocampal memory prior to consolidation, I've just shown that the generative model can learn the tasks (through causal sequence modelling, i.e. 'prediction error minimisation' on stored 'memories'). As discussed you might expect reward revaluation soon after encoding to be easier - you can retrieve a (non-consolidated) sequence from HPC then infer the rewards for a different reward category, whereas transition revaluation requires knowledge of transition probabilities to be extracted from the memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1a12d-ed03-4a17-960d-0a6863681170",
   "metadata": {},
   "source": [
    "#### Installation / imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05917c15-2dc4-4d88-9ab0-5135187faaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers\n",
    "! pip install accelerate evaluate wonderwords simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7afc1-f112-4b77-800e-b2f47f67e159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "from wonderwords import RandomWord\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# import functions from another Python file in my code:\n",
    "from gpt import GPT\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464ff8e-5309-483a-bd83-0a28dfbba60d",
   "metadata": {},
   "source": [
    "#### Pre-train model on arbitrary stimuli to learn rules of task\n",
    "\n",
    "The get_random_stimuli() function generates a random set of nouns and adjectives (3 for each by default). The stimuli are all possible combinations, e.g. for the adjectives ABC and nouns DEF, the stimuli are AD, AE, AF, BD, etc. The get_stimuli() function is the equivalent but for Oliver's task stimuli. \n",
    "\n",
    "The get_reward() function predicts reward points for a sequence of stimuli. Given a list of stimuli in random order, the stimulus at which the sequence starts, the adjective at which the sequence ends, and the noun that gives 2 points of reward, the function returns a list of stimuli and their rewards, e.g. ['small chair (2)', 'angry chair (2)', 'metal spoon (-1)']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90dc38-13d5-4915-b7ab-66550bf55c80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_stimuli(n=3):\n",
    "    r = RandomWord()\n",
    "    adjectives = [r.word(include_parts_of_speech=[\"adjectives\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "    nouns = [r.word(include_parts_of_speech=[\"nouns\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "\n",
    "    stimuli = []\n",
    "    for i, noun in enumerate(nouns):\n",
    "        for adjective in adjectives:\n",
    "            stimuli.append(f\"{adjective} {noun}\")\n",
    "\n",
    "    return stimuli, nouns, adjectives\n",
    "\n",
    "def get_stimuli():\n",
    "    stimuli = [\"red animal\", \n",
    "               \"green animal\", \n",
    "               \"yellow animal\", \n",
    "               \"red vehicle\", \n",
    "               \"green vehicle\", \n",
    "               \"yellow vehicle\", \n",
    "               \"red fruit\", \n",
    "               \"green fruit\", \n",
    "               \"yellow fruit\"]\n",
    "    \n",
    "    objects = [word.split()[1] for word in stimuli]\n",
    "    colours = list(set([word.split()[0] for word in stimuli]))\n",
    "    return stimuli, objects, colours\n",
    "\n",
    "def shuffle_stimuli(stimuli):\n",
    "    random.shuffle(stimuli)\n",
    "    return stimuli\n",
    "\n",
    "def get_reward(stimuli, start, stop, reward):\n",
    "    \"\"\"Predict reward points for a sequence of stimuli.\n",
    "    \n",
    "    Args:\n",
    "        stimuli (list): List of stimuli in random order.\n",
    "        start (str): Object at which the sequence starts.\n",
    "        stop (str): Colour at which the sequence ends.\n",
    "        reward (str): Category of object that brings 2 points of reward.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Reward points descriptions for the sequence.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    sequence_started = False\n",
    "    \n",
    "    index = 0  # start index\n",
    "    while True:\n",
    "        stim = stimuli[index % len(stimuli)]\n",
    "        colour, obj = stim.split()\n",
    "\n",
    "        if not sequence_started:\n",
    "            if stim == start:\n",
    "                sequence_started = True\n",
    "            else:\n",
    "                index += 1\n",
    "                continue\n",
    "\n",
    "        if obj == reward:\n",
    "            points.append(f\"{stim} (2)\")\n",
    "        else:\n",
    "            points.append(f\"{stim} (-1)\")\n",
    "\n",
    "        if colour == stop:\n",
    "            break\n",
    "\n",
    "        index += 1  # move to the next stimulus\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83366812-1e46-4304-a690-d3cadf2cf35e",
   "metadata": {},
   "source": [
    "We now generate training and test data for the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b93d3f-dd38-40bf-a3e5-cfce37e89d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_strs = []\n",
    "for i in range(300):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                training_strs.append(training_str)\n",
    "\n",
    "testing_strs = []\n",
    "for i in range(10):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                testing_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                testing_strs.append(testing_str)\n",
    "\n",
    "print(f\"{len(training_strs)} sequences of arbitrary stimuli generated for pre-training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a49df-cb78-4b49-aa1e-6f894f48ca63",
   "metadata": {},
   "source": [
    "The function below runs a script to fine-tune a gpt-2 model on the arbitrary stimuli.\n",
    "\n",
    "The name_or_path argument is which model to fine-tune from. In the pre-training stage, this will be set to 'gpt2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2433c5-aa7c-496c-a5b7-849674956a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=3,\n",
    "                       output_dir='./clm_script'):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ! python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --validation_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps 100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e5c52-5e6a-4769-983b-23a1117d1c08",
   "metadata": {},
   "source": [
    "Shuffle the data, write it to train.txt and test.txt files, and train gpt2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039427a-4609-4b49-b39c-ce4cbedccaf9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf rule_model\n",
    "!mkdir rule_model\n",
    "\n",
    "# random.shuffle(training_strs)\n",
    "# random.shuffle(testing_strs)\n",
    "\n",
    "text_file = open(\"rule_model/train.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(training_strs))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"rule_model/test.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(testing_strs))\n",
    "text_file.close()\n",
    "\n",
    "train_model_script(name_or_path='gpt2', output_dir='rule_model', num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a9fe0-84c8-4d80-b51f-1cc503980685",
   "metadata": {},
   "source": [
    "Test the output with some random start / stop / reward conditions not seen in the training data to see if it's learned the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d33e27-6eb0-45a7-98e1-bb9d6eee3f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT(base_model='rule_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dbdfa-a65b-4033-bbd0-455d74eff664",
   "metadata": {},
   "source": [
    "Can the model generalise the rules to new stimuli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99c4b7-f598-45b5-b891-6c808a1836fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model.continue_input(\"START: blue chair, STOP: sad, REWARD: bug, SEQUENCE:\", do_sample=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4085-ea90-44aa-b7d1-fb72b9e1d614",
   "metadata": {},
   "source": [
    "#### 'Retrieval augmented generation'\n",
    "\n",
    "Can the generative network's outputs be conditioned on hippocampal sequences (inspired by retrieval augmented generation), to support some degree of generalisation soon after encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7df9c-2a09-4f6b-b660-8d6176b3a48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def get_seqs_RAG(base_dir='clm_script_0', test_type='reward'):\n",
    "    with open(os.path.join(base_dir, f'test_{test_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        test_seqs = list(set(seqs))\n",
    "\n",
    "    with open(os.path.join(base_dir, f'train.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        train_seqs = list(set(seqs))\n",
    "    \n",
    "    return train_seqs, test_seqs\n",
    "\n",
    "def reward_test_RAG(base_dir = 'clm_script_0'):\n",
    "    train_seqs, test_seqs = get_seqs_RAG(base_dir=base_dir, test_type='reward')\n",
    "\n",
    "    if 'REWARD: fruit' in ''.join(test_seqs):\n",
    "        reward = 'fruit'\n",
    "    if 'REWARD: vehicle' in ''.join(test_seqs):\n",
    "        reward = 'vehicle'\n",
    "    if 'REWARD: animal' in ''.join(test_seqs):\n",
    "        reward = 'animal'\n",
    "    print(reward)\n",
    "\n",
    "    result_bools = []\n",
    "    # reward revaluation test:\n",
    "    for test in test_seqs:\n",
    "        # print(test)\n",
    "        partial_test = test[0:test.index('REWARD')]\n",
    "        train_examples = [t for t in train_seqs if partial_test in t][0:1]\n",
    "        continuation = model.continue_input(\"\\n\".join(train_examples) + '\\n' + partial_test + f'REWARD: {reward}, SEQUENCE:',\n",
    "                                            do_sample=True)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', len(train_examples) + 1)\n",
    "        end_ind = find_nth(continuation, 'START', len(train_examples) + 2)\n",
    "        \n",
    "        true_a_v_r = get_accept_reject_choice(test)\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            result_bools.append(0)\n",
    "    return result_bools\n",
    "        \n",
    "def transition_test_RAG(base_dir = 'clm_script_0'):\n",
    "    train_seqs, test_seqs = get_seqs_RAG(base_dir=base_dir, test_type='transition')\n",
    "\n",
    "    if 'STOP: red' in ''.join(test_seqs):\n",
    "        stop = 'red'\n",
    "    if 'STOP: green' in ''.join(test_seqs):\n",
    "        stop = 'green'\n",
    "    if 'STOP: yellow' in ''.join(test_seqs):\n",
    "        stop = 'yellow'\n",
    "    print(stop)\n",
    "\n",
    "    result_bools = []\n",
    "    # transition revaluation test:\n",
    "    for test in test_seqs:\n",
    "        # print(test)\n",
    "        start_criterion = test[test.index('START'):test.index('STOP')]\n",
    "        reward_criterion = test[test.index('REWARD'):test.index('SEQUENCE')]\n",
    "        train_examples = [t for t in train_seqs if start_criterion in t and reward_criterion in t]\n",
    "        continuation = model.continue_input(\"\\n\".join(train_examples) + '\\n' + test[0:test.index('SEQUENCE')] + 'SEQUENCE:',\n",
    "                                            do_sample=True)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', len(train_examples) + 1)\n",
    "        end_ind = find_nth(continuation, 'START', len(train_examples) + 2)\n",
    "\n",
    "        true_a_v_r = get_accept_reject_choice(test)\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            result_bools.append(0)\n",
    "    return result_bools\n",
    "\n",
    "all_transition = []\n",
    "all_reward = []\n",
    "for i in range(10):\n",
    "    res = transition_test_RAG(base_dir = f'clm_script_{i}')\n",
    "    all_transition.append(res)\n",
    "    res = reward_test_RAG(base_dir = f'clm_script_{i}')\n",
    "    all_reward.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace52a73-292b-4b50-bbb6-14790ec32940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(all_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4679a53-74b1-48de-a9a7-12a6d5934fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(all_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9822f-e4e1-4d7f-a468-54d557d7cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(['Transition revaluation', 'Reward revaluation'], \n",
    "        [np.mean(all_transition), np.mean(all_reward)])\n",
    "\n",
    "plt.ylabel('Accept / reject accuracy')\n",
    "plt.title('Recall pre-consolidation')\n",
    "plt.savefig('rag.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb1926-5a4a-49d8-9a91-cd8a80410df0",
   "metadata": {},
   "source": [
    "#### Simulate the task\n",
    "\n",
    "The simulate_task() function runs one trial, for either the transition or reward revaluation task (as set by the leave_out argument):\n",
    "* First it clears down the model directory.\n",
    "* Then it shuffles the stimuli for the trial into a random order.\n",
    "* Then it generates all possible start / stop / reward combinations and gets the sequence for each from the get_reward() function.\n",
    "* As above, it turns these into strings with format f\"START: ..., STOP: ..., REWARD: ..., SEQUENCE: ...\"\n",
    "* In the reward revaluation case, we train on 2/3 of the reward categories in each trial, and test on a held-out reward category. In the transition revaluation case, we train on 2/3 of the stop colours in each trial, and test on a held-out transition category. \n",
    "* The training and test data is saved to files, and a model is trained by calling the train_model_script() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75e56c-47d1-408b-8eae-244d03808b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_task(leave_out='transition', seed=0):\n",
    "    random.seed(seed)\n",
    "    training_strs = []\n",
    "    \n",
    "    # get stimuli etc and shuffle them\n",
    "    stimuli, objects, colours = get_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    \n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                training_strs.append(training_str)\n",
    "\n",
    "    if leave_out == 'both':\n",
    "        random_category = random.choice(objects)\n",
    "        random_colour = random.choice(colours)\n",
    "        train_set = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' not in s]\n",
    "        test_set_reward = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' not in s]\n",
    "        test_set_transition = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' in s]\n",
    "        test_set_both = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' in s]\n",
    "\n",
    "        # oversampling trick to avoid overfitting to sequence order\n",
    "        train_set = np.random.choice(train_set, 10000).tolist()\n",
    "        \n",
    "        output_dir = f'clm_script_{seed}'\n",
    "        ! rm -rf {output_dir}\n",
    "        !mkdir {output_dir}\n",
    "        \n",
    "        text_file = open(os.path.join(output_dir, 'train.txt'), \"w\")\n",
    "        n = text_file.write('\\n'.join(train_set))\n",
    "        text_file.close()\n",
    "\n",
    "        text_file = open(os.path.join(output_dir, 'test_reward.txt'), \"w\")\n",
    "        n = text_file.write('\\n'.join(test_set_reward))\n",
    "        text_file.close()\n",
    "        \n",
    "        text_file = open(os.path.join(output_dir, 'test_transition.txt'), \"w\")\n",
    "        n = text_file.write('\\n'.join(test_set_transition))\n",
    "        text_file.close()\n",
    "\n",
    "        text_file = open(os.path.join(output_dir, 'test_both.txt'), \"w\")\n",
    "        n = text_file.write('\\n'.join(test_set_both))\n",
    "        text_file.close()\n",
    "\n",
    "    else:\n",
    "        if leave_out == 'transition':\n",
    "            random_colour = random.choice(colours)\n",
    "            train_set = [s for s in training_strs if f'STOP: {random_colour}' not in s]\n",
    "            test_set = [s for s in training_strs if f'STOP: {random_colour}' in s]\n",
    "        if leave_out == 'reward':\n",
    "            random_category = random.choice(objects)\n",
    "            train_set = [s for s in training_strs if f'REWARD: {random_category}' not in s]\n",
    "            test_set = [s for s in training_strs if f'REWARD: {random_category}' in s]\n",
    "        \n",
    "        # oversampling trick to avoid overfitting to sequence order\n",
    "        train_set = np.random.choice(train_set, 10000).tolist()\n",
    "        test_set = np.random.choice(test_set, 10000).tolist()\n",
    "\n",
    "        text_file = open(\"train.txt\", \"w\")\n",
    "        n = text_file.write('\\n'.join(train_set))\n",
    "        text_file.close()\n",
    "\n",
    "        text_file = open(\"test.txt\", \"w\")\n",
    "        n = text_file.write('\\n'.join(test_set))\n",
    "        text_file.close()\n",
    "    \n",
    "    ! rm -rf clm_script\n",
    "    train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=2, \n",
    "                       output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4b845-6a51-47b0-a593-ec90a2b62fcc",
   "metadata": {},
   "source": [
    "Reward and transition revaluation test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39c98e-0d49-4f6d-be58-cce58da8a4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accept_reject_choice(seq):\n",
    "    seq = seq[seq.index('SEQUENCE:') + len('SEQUENCE:'):]\n",
    "    if 'START' in seq:\n",
    "        seq = seq[0:seq.index('START')]\n",
    "    print(seq)\n",
    "    numbers = re.findall(r'\\([A-Za-z0-9_-]+\\)', seq)\n",
    "    numbers = [int(num.replace('(', '').replace(')', '')) for num in numbers]\n",
    "    print(numbers)\n",
    "    if sum(numbers) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def test_revaluation(seqs, model):\n",
    "    result_bools = []\n",
    "    result_preds = []\n",
    "    for seq in seqs:\n",
    "        print(\"Get true accept / reject:\")\n",
    "        true_a_v_r = get_accept_reject_choice(seq)\n",
    "        print(true_a_v_r)\n",
    "        input_str = seq[0:seq.index('SEQUENCE:') + len('SEQUENCE:')]\n",
    "        continuation = model.continue_input(input_str, do_sample=False)\n",
    "        print(\"Get pred accept / reject:\")\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation)\n",
    "        print(pred_a_v_r)\n",
    "        result_preds.append(continuation)\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            print(\"match\")\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            print(\"no match\")\n",
    "            result_bools.append(0)\n",
    "    return (result_bools, result_preds)\n",
    "\n",
    "def get_len(seq):\n",
    "    return seq.count(',') - 2\n",
    "\n",
    "def get_mean(val):\n",
    "    return np.mean(val[0])\n",
    "\n",
    "def num_sort(test_string):\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]\n",
    "\n",
    "def test_consolidation(base_dir, task_type='transition', both=True):\n",
    "    checkpoints = glob.glob(os.path.join(base_dir, 'checkpoint*'))\n",
    "    model_dirs = ['rule_model'] + checkpoints \n",
    "    print(model_dirs)\n",
    "\n",
    "    with open(os.path.join(base_dir, f'test_{task_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        seqs = list(set(seqs))\n",
    "\n",
    "    results = {}\n",
    "    for model_dir in model_dirs:\n",
    "        print(model_dir)\n",
    "        model = GPT(base_model=model_dir, base_model_name='gpt2-medium')\n",
    "        results[os.path.basename(model_dir)] = test_revaluation(seqs, model)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1934a6-0771-460e-897b-75214b053cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    simulate_task(leave_out='both', seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1dd32-6cf2-4ffb-9336-47aecea4a9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_results_transition = []\n",
    "all_results_reward = []\n",
    "all_results_both = []\n",
    "\n",
    "for i in range(10):\n",
    "    base_dir = f'clm_script_{i}'\n",
    "    \n",
    "    results = test_consolidation(base_dir, task_type='transition')\n",
    "    all_results_transition.append(results)\n",
    "\n",
    "    results = test_consolidation(base_dir, task_type='reward')\n",
    "    all_results_reward.append(results)\n",
    "\n",
    "    results = test_consolidation(base_dir, task_type='both')\n",
    "    all_results_both.append(results)\n",
    "\n",
    "with open('results_transition.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_transition, f)\n",
    "with open('results_reward.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_reward, f)\n",
    "with open('results_both.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_both, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87448-18f2-4fb4-b2b9-a46ba8603c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(file_name, errors=False):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.dropna(axis='columns')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(get_mean)\n",
    "\n",
    "    stats_to_plot = df.describe()\n",
    "    \n",
    "    if errors is False:\n",
    "        return stats_to_plot['rule_model']['mean'], stats_to_plot['checkpoint-100']['mean'], stats_to_plot['checkpoint-200']['mean'], stats_to_plot['checkpoint-300']['mean'], stats_to_plot['checkpoint-400']['mean'], stats_to_plot['checkpoint-500']['mean']\n",
    "    if errors is True:\n",
    "        return stats_to_plot['rule_model']['std'], stats_to_plot['checkpoint-100']['std'], stats_to_plot['checkpoint-200']['std'], stats_to_plot['checkpoint-300']['std'], stats_to_plot['checkpoint-400']['std'], stats_to_plot['checkpoint-500']['std']\n",
    "    \n",
    "all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500 = get_data_from_file('results_both.pkl')\n",
    "plt.plot([0, 100, 200, 300, 400, 500], \n",
    "        [all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500],\n",
    "        marker='o',\n",
    "        label='Reward and transition revaluation')\n",
    "\n",
    "all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500 = get_data_from_file('results_reward.pkl')\n",
    "plt.plot([0, 100, 200, 300, 400, 500], \n",
    "        [all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500],\n",
    "        marker='o',\n",
    "        label='Reward revaluation')\n",
    "\n",
    "all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500 = get_data_from_file('results_transition.pkl')\n",
    "plt.plot([0, 100, 200, 300, 400, 500], \n",
    "        [all_pre, checkpoint100, checkpoint200, checkpoint300, checkpoint400, checkpoint500],\n",
    "        marker='o',\n",
    "        label='Transition revaluation')\n",
    "\n",
    "plt.yticks(np.linspace(0, 1, num=11))\n",
    "plt.ylabel('Accept / reject accuracy')\n",
    "plt.xlabel('Training steps')\n",
    "plt.title('Transition and reward revaluation')\n",
    "plt.legend()\n",
    "plt.savefig('revaluation over time 4.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba8f86-71f1-4c9b-a631-ba52e38042ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
