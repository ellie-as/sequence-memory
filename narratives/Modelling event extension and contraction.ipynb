{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9268b06-9d54-4194-be41-53315d3f5990",
   "metadata": {
    "id": "d9268b06-9d54-4194-be41-53315d3f5990"
   },
   "source": [
    "### Modelling distortions in narratives\n",
    "\n",
    "An (overfitted) transformer-based model such as GPT-2 can memorise its training data. Here we explore distortions in the resulting model when trained on narratives, comparing the results to Raykov et al. (2023).\n",
    "\n",
    "Three types of story from the ROC Stories dataset are consolidated into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700b9b2-a25b-41bc-aa0d-711f770d37ea",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36412af-d07d-4ab6-b477-26bf71619a6a",
   "metadata": {
    "id": "b36412af-d07d-4ab6-b477-26bf71619a6a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import random\n",
    "from story_utils import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pointbiserialr\n",
    "from scipy.stats import t\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ba38e-3250-4730-8e0d-9c8cf82e3a56",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iTW5f1i05XzL",
   "metadata": {
    "id": "iTW5f1i05XzL"
   },
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "\n",
    "    def continue_input(self, input_sequence, max_length=200, num_return_sequences=1, no_repeat_ngram_size=10,\n",
    "                       do_sample=False, temperature=0, num_beams=1):\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "        max_length = len(input_ids[0]) + 100\n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad640853-6074-4b6d-a3e9-cb37d55985e3",
   "metadata": {
    "id": "ad640853-6074-4b6d-a3e9-cb37d55985e3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(num_typical=100, num_char=50, num_variants=20):\n",
    "    stories = get_stories()\n",
    "    shuffle(stories)\n",
    "    lengths = [len(s) for s in stories]\n",
    "    mean_len = np.mean(lengths)\n",
    "    mean_len = int(mean_len)\n",
    "    print(mean_len)\n",
    "\n",
    "    typical = stories[:num_typical]\n",
    "    lengthened = []\n",
    "    shortened = []\n",
    "    for s in stories[num_typical:]:\n",
    "        if len(s) < mean_len:\n",
    "            delta = mean_len - len(s)\n",
    "            # increase length up to average\n",
    "            if delta > 100:\n",
    "                new_s = s + get_random_sentence(stories)[0:delta]\n",
    "                lengthened.append(new_s)\n",
    "        if len(s) > mean_len:\n",
    "            # decrease length down to average\n",
    "            delta = len(s) - mean_len\n",
    "            if delta > 100:\n",
    "                new_s = s[0:mean_len]\n",
    "                shortened.append(new_s)\n",
    "\n",
    "    stories = [s + ' The end.' for s in stories]\n",
    "    shortened = [s + ' The end.' for s in shortened]\n",
    "    lengthened = [s + ' The end.' for s in lengthened]\n",
    "    return stories[:num_typical], shortened[:num_variants], lengthened[:num_variants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d66d5-6f88-4666-8a12-5007ba169e31",
   "metadata": {
    "id": "803d66d5-6f88-4666-8a12-5007ba169e31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_length_difference(stories):\n",
    "    \"\"\"\n",
    "    Computes the length difference between input and output for a given list of stories.\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "    for story in stories:\n",
    "        input_length = len(story[0])\n",
    "        output_length = len(story[1])\n",
    "        difference = output_length - input_length\n",
    "        differences.append(difference)\n",
    "        print(difference)\n",
    "    return sum(differences) / len(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a9cdc-3cb5-4455-88bc-02c19d38756b",
   "metadata": {
    "id": "e29a9cdc-3cb5-4455-88bc-02c19d38756b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(save_name, typical, atypical_short, atypical_long):\n",
    "\n",
    "    model = GPT(base_model='outputs_stories')\n",
    "\n",
    "    results_dict = {}\n",
    "    results_dict['typical'] = []\n",
    "    results_dict['atypical_short'] = []\n",
    "    results_dict['atypical_long'] = []\n",
    "\n",
    "    for s in typical[0:len(atypical_short)]:\n",
    "        start = \" \".join(s.split()[0:10])\n",
    "        gen = model.continue_input(start)\n",
    "        if 'The end.' in gen:\n",
    "            gen = gen[0:gen.index('The end.')+8]\n",
    "            print(f\"START: \\n{start}\")\n",
    "            print(f\"GENERATED: \\n{gen}\")\n",
    "            print(f\"TRUE: \\n{s}\")\n",
    "            results_dict['typical'].append([s, gen])\n",
    "\n",
    "    for s in atypical_short:\n",
    "        start = \" \".join(s.split()[0:10])\n",
    "        gen = model.continue_input(start)\n",
    "        if 'The end.' in gen:\n",
    "            gen = gen[0:gen.index('The end.')+8]\n",
    "            print(f\"START: \\n{start}\")\n",
    "            print(f\"GENERATED: \\n{gen}\")\n",
    "            print(f\"TRUE: \\n{s}\")\n",
    "            results_dict['atypical_short'].append([s, gen])\n",
    "\n",
    "    for s in atypical_long:\n",
    "        start = \" \".join(s.split()[0:10])\n",
    "        gen = model.continue_input(start)\n",
    "        if 'The end.' in gen:\n",
    "            gen = gen[0:gen.index('The end.')+8]\n",
    "            print(f\"START: \\n{start}\")\n",
    "            print(f\"GENERATED: \\n{gen}\")\n",
    "            print(f\"TRUE: \\n{s}\")\n",
    "            results_dict['atypical_long'].append([s, gen])\n",
    "\n",
    "\n",
    "    # Calculate the average length difference for each category\n",
    "    typical_difference = compute_length_difference(results_dict['typical'])\n",
    "    atypical_short_difference = compute_length_difference(results_dict['atypical_short'])\n",
    "    atypical_long_difference = compute_length_difference(results_dict['atypical_long'])\n",
    "\n",
    "    # Plotting the results\n",
    "    categories = ['Atypical Short', 'Typical', 'Atypical Long']\n",
    "    differences = [atypical_short_difference, typical_difference, atypical_long_difference]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(categories, differences)\n",
    "    plt.xlabel('Story Category')\n",
    "    plt.ylabel('Average Length Difference (Output - Input)')\n",
    "    plt.title('Length Difference by Story Category')\n",
    "    plt.axhline(y=0, color='black')\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    with open(save_name + '.pkl', 'wb') as handle:\n",
    "        pickle.dump(results_dict, handle)\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70157ee0-1197-4d0e-989f-0d5c7d516970",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a078f4-e102-4b4e-bd08-f715ccd1d1fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "07a078f4-e102-4b4e-bd08-f715ccd1d1fb",
    "outputId": "1aab73ed-1080-491a-ad78-010c7a8b1580",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='gpt2-medium',\n",
    "                       num_epochs=3,\n",
    "                       output_dir='outputs_stories',\n",
    "                       save_steps=1000,\n",
    "                       lr=5e-04):\n",
    "    gc.collect()\n",
    "    ! python3 run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {'./outputs_stories/train.txt'} \\\n",
    "        --validation_file {'./outputs_stories/train.txt'} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr} \\\n",
    "        --report_to 'none'\n",
    "\n",
    "for i in range(0, 10):\n",
    "    for num_typical in [100]:\n",
    "        for num_char in [100]:\n",
    "            for num_variants in [10]:\n",
    "                for num_eps in [5]:\n",
    "\n",
    "                    !rm -rf outputs_stories\n",
    "                    !mkdir outputs_stories\n",
    "\n",
    "                    typical, atypical_short, atypical_long = prepare_data(num_typical=num_typical,\n",
    "                                                                          num_variants=num_variants)\n",
    "                    sents_list = typical + atypical_short + atypical_long\n",
    "                    #sents_list = np.random.choice(sents_list, 100).tolist()\n",
    "                    shuffle(sents_list)\n",
    "\n",
    "                    with open(\"outputs_stories/train.txt\", \"w\") as fh:\n",
    "                        fh.write('\\n'.join(sents_list))\n",
    "\n",
    "                    with open(\"outputs_stories/test.txt\", \"w\") as fh:\n",
    "                        fh.write('\\n'.join(sents_list))\n",
    "\n",
    "                    train_model_script(num_epochs=num_eps)\n",
    "\n",
    "                    test_model(f'./plots/{i}trial_{num_eps}epochs_{num_typical}typicals_{num_char}chars_{num_variants}variants.png',\n",
    "                              typical,\n",
    "                              atypical_short,\n",
    "                              atypical_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470ffc2-c72a-4735-9af4-c374c558ae8b",
   "metadata": {},
   "source": [
    "#### Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144822d-f47b-4b06-a224-9f817e3c720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare lengths of input and output\n",
    "def length_comparison(category_data):\n",
    "    shorter, same, longer = 0, 0, 0\n",
    "    total_stories = 10  # Each .pkl file has exactly 10 stories in each category\n",
    "    \n",
    "    for input_text, output_text in category_data:\n",
    "        if len(output_text) < len(input_text):\n",
    "            shorter += 1\n",
    "        elif len(output_text) == len(input_text):\n",
    "            same += 1\n",
    "        else:\n",
    "            longer += 1\n",
    "    \n",
    "    # Ensure we divide by 10 to get fractions (since each .pkl has 10 stories)\n",
    "    return shorter / total_stories, same / total_stories, longer / total_stories  # Return fractions\n",
    "\n",
    "# Function to calculate 95% confidence intervals\n",
    "def calculate_95_ci(data):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = np.std(data, ddof=1) / np.sqrt(n)  # Standard error of the mean\n",
    "    ci_range = t.ppf(0.975, df=n-1) * sem    # 95% CI (two-tailed, so 0.975 for upper bound)\n",
    "    return mean, ci_range\n",
    "\n",
    "# Key remapping dictionary\n",
    "key_mapping = {\n",
    "    'typical': 'typical',\n",
    "    'atypical_short': 'incomplete',\n",
    "    'atypical_long': 'updated'\n",
    "}\n",
    "\n",
    "# Categories to compare\n",
    "categories = ['incomplete', 'updated']\n",
    "\n",
    "# Initialize lists to store results for each individual\n",
    "individual_results = {'incomplete': {'shorter': [], 'longer': []},\n",
    "                      'updated': {'shorter': [], 'longer': []}}\n",
    "\n",
    "# Load each .pkl file separately, treating each as an individual\n",
    "for pkl in glob.glob('event_data/*.pkl'):\n",
    "    with open(pkl, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    \n",
    "    # Remap the keys from 'typical', 'atypical_short', 'atypical_long' to 'typical', 'incomplete', 'updated'\n",
    "    remapped_data = {key_mapping[k]: v for k, v in d.items()}\n",
    "    \n",
    "    # Calculate fractions for each category for this individual\n",
    "    for category in categories:\n",
    "        shorter, _, longer = length_comparison(remapped_data.get(category, []))\n",
    "        individual_results[category]['shorter'].append(shorter)\n",
    "        individual_results[category]['longer'].append(longer)\n",
    "\n",
    "# Convert individual results to arrays for easier manipulation\n",
    "incomplete_shorter = np.array(individual_results['incomplete']['shorter'])\n",
    "incomplete_longer = np.array(individual_results['incomplete']['longer'])\n",
    "updated_shorter = np.array(individual_results['updated']['shorter'])\n",
    "updated_longer = np.array(individual_results['updated']['longer'])\n",
    "\n",
    "# Calculate means and 95% CI for each error type\n",
    "mean_incomplete_shorter, ci_incomplete_shorter = calculate_95_ci(incomplete_shorter)\n",
    "mean_incomplete_longer, ci_incomplete_longer = calculate_95_ci(incomplete_longer)\n",
    "mean_updated_shorter, ci_updated_shorter = calculate_95_ci(updated_shorter)\n",
    "mean_updated_longer, ci_updated_longer = calculate_95_ci(updated_longer)\n",
    "\n",
    "means_incomplete = [mean_incomplete_shorter, mean_incomplete_longer]\n",
    "means_updated = [mean_updated_shorter, mean_updated_longer]\n",
    "\n",
    "ci_incomplete = [ci_incomplete_shorter, ci_incomplete_longer]\n",
    "ci_updated = [ci_updated_shorter, ci_updated_longer]\n",
    "\n",
    "labels = ['Omission errors', 'Extension errors']\n",
    "x = np.arange(len(labels)) \n",
    "bar_width = 0.35 \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.3, 2.5))\n",
    "ax.bar(x - bar_width/2, means_incomplete, bar_width, yerr=ci_incomplete, color='skyblue', alpha=1, label='Incomplete', capsize=5)\n",
    "ax.bar(x + bar_width/2, means_updated, bar_width, yerr=ci_updated, color='purple', alpha=0.6, label='Updated', capsize=5)\n",
    "\n",
    "# Plot individual points with optional jitter offset\n",
    "jitter_amount=0.00\n",
    "ax.scatter(x[0] - bar_width/2 + np.random.uniform(-jitter_amount, jitter_amount, len(incomplete_shorter)), \n",
    "           incomplete_shorter, color='skyblue', s=30, edgecolors='darkgrey', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.scatter(x[1] - bar_width/2 + np.random.uniform(-jitter_amount, jitter_amount, len(incomplete_longer)), \n",
    "           incomplete_longer, color='skyblue', s=30, edgecolors='darkgrey', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.scatter(x[0] + bar_width/2 + np.random.uniform(-jitter_amount, jitter_amount, len(updated_shorter)), \n",
    "           updated_shorter, color='purple', s=30, edgecolors='darkgrey', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.scatter(x[1] + bar_width/2 + np.random.uniform(-jitter_amount, jitter_amount, len(updated_longer)), \n",
    "           updated_longer, color='purple', s=30, edgecolors='darkgrey', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Error type')\n",
    "ax.set_ylabel('Proportion of errors')\n",
    "ax.set_xticks(x)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('omission_vs_extension_with_individuals_95CI.png', bbox_inches='tight', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede10046-e153-4099-b91e-958b83f50548",
   "metadata": {},
   "source": [
    "Extra plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d660dc3-fb9a-436f-8dae-8ca964b5b932",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "5d660dc3-fb9a-436f-8dae-8ca964b5b932",
    "outputId": "3925f356-06e1-4c2a-d529-e2e77e2577f0"
   },
   "outputs": [],
   "source": [
    "def compute_length_difference_and_sem(stories):\n",
    "    \"\"\"\n",
    "    Computes the length difference and SEM between input and output for a given list of stories.\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "    for story in stories:\n",
    "        input_length = len(story[0])\n",
    "        output_length = len(story[1])\n",
    "        difference = output_length - input_length\n",
    "        differences.append(difference)\n",
    "    average_difference = sum(differences) / len(differences)\n",
    "    # Calculate SEM\n",
    "    sem = np.std(differences, ddof=1) / np.sqrt(len(differences))\n",
    "    return average_difference, sem\n",
    "\n",
    "combined = {'typical': [], 'atypical_short': [], 'atypical_long': []}\n",
    "for pkl in glob.glob('event_data/*.pkl'):\n",
    "    with open(pkl, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    combined['typical'].extend(d['typical'])\n",
    "    combined['atypical_long'].extend(d['atypical_long'])\n",
    "    combined['atypical_short'].extend(d['atypical_short'])\n",
    "\n",
    "# Calculate the average length difference and SEM for each category\n",
    "typical_difference, typical_sem = compute_length_difference_and_sem(combined['typical'])\n",
    "atypical_short_difference, atypical_short_sem = compute_length_difference_and_sem(combined['atypical_short'])\n",
    "atypical_long_difference, atypical_long_sem = compute_length_difference_and_sem(combined['atypical_long'])\n",
    "\n",
    "# Plotting the results with error bars\n",
    "categories = ['Incomplete', 'Complete', 'Updated']\n",
    "differences = [atypical_short_difference, typical_difference, atypical_long_difference]\n",
    "sems = [atypical_short_sem, typical_sem, atypical_long_sem]  # SEMs for error bars\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(categories, differences, yerr=sems, capsize=5, alpha=0.5)  # Add error bars with capsize\n",
    "plt.xlabel('Story Category')\n",
    "plt.ylabel('Length Difference')\n",
    "plt.axhline(y=0, color='black')\n",
    "plt.savefig('event_ext.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
