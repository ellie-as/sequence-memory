{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb8ddc3",
   "metadata": {
    "id": "cbb8ddc3"
   },
   "source": [
    "### Exploring distortion over time in the Bartlett experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350cae9",
   "metadata": {
    "id": "6350cae9"
   },
   "source": [
    "#### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcda51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61bcda51",
    "outputId": "6b9ed565-884e-4468-8afd-49e7faf62352",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud datasets evaluate accelerate simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7822371",
   "metadata": {
    "id": "b7822371"
   },
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb64462",
   "metadata": {
    "id": "7eb64462"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from wordcloud import WordCloud\n",
    "import gc\n",
    "from random import shuffle\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25HoKEIa0XEJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25HoKEIa0XEJ",
    "outputId": "d710eebf-779f-4107-aa77-7273d0404664"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VImSeIqF9SWQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VImSeIqF9SWQ",
    "outputId": "349d7e4c-97c7-4c11-b516-a892da1d8992"
   },
   "outputs": [],
   "source": [
    "!wandb disabled\n",
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901baf23-bd58-405b-8e90-95b3c3c81a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for results\n",
    "base_dir = '/content/drive/MyDrive/colab_code/outputs2510'\n",
    "# Which topic(s) to run analysis for\n",
    "topics = ['Nature'] #['Universe', 'Politics', 'Health', 'Sport', 'Technology', 'Nature']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4239b6-671f-45f1-88a3-d4de0e70b05a",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tTId8Ydir_Y",
   "metadata": {
    "id": "6tTId8Ydir_Y"
   },
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def continue_input(self, input_sequence, max_length=200, num_return_sequences=1, no_repeat_ngram_size=10,\n",
    "                       do_sample=False, temperature=0, num_beams=1):\n",
    "\n",
    "        # Tokenize and move input to GPU if available\n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295764ce",
   "metadata": {
    "id": "295764ce"
   },
   "outputs": [],
   "source": [
    "bartlett = \"\"\"One night two young men from Egulac went down to the river to hunt seals and while they were there it became foggy and calm. Then they heard war-cries, and they thought: \"Maybe this is a war-party\". They escaped to the shore, and hid behind a log. Now canoes came up, and they heard the noise of paddles, and saw one canoe coming up to them. There were five men in the canoe, and they said:\n",
    "\"What do you think? We wish to take you along. We are going up the river to make war on the people.\"\n",
    "One of the young men said,\"I have no arrows.\"\n",
    "\"Arrows are in the canoe,\" they said.\n",
    "\"I will not go along. I might be killed. My relatives do not know where I have gone. But you,\" he said, turning to the other, \"may go with them.\"\n",
    "So one of the young men went, but the other returned home.\n",
    "And the warriors went on up the river to a town on the other side of Kalama. The people came down to the water and they began to fight, and many were killed. But presently the young man heard one of the warriors say, \"Quick, let us go home: that man has been hit.\" Now he thought: \"Oh, they are ghosts.\" He did not feel sick, but they said he had been shot.\n",
    "So the canoes went back to Egulac and the young man went ashore to his house and made a fire. And he told everybody and said: \"Behold I accompanied the ghosts, and we went to fight. Many of our fellows were killed, and many of those who attacked us were killed. They said I was hit, and I did not feel sick.\"\n",
    "He told it all, and then he became quiet. When the sun rose he fell down. Something black came out of his mouth. His face became contorted. The people jumped up and cried.\n",
    "He was dead.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e7bf5-6b72-45dd-b037-f81744c7330a",
   "metadata": {
    "id": "038e7bf5-6b72-45dd-b037-f81744c7330a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='openai-community/gpt2-medium',\n",
    "                       num_epochs=50,\n",
    "                       output_dir='bartlett',\n",
    "                       save_steps=200,\n",
    "                       lr=5e-04):\n",
    "    gc.collect()\n",
    "    train_path = f'{output_dir}/train.txt'\n",
    "    ! python run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {train_path} \\\n",
    "        --validation_file {train_path} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WkYp9judxsBB",
   "metadata": {
    "id": "WkYp9judxsBB"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('tarekziade/wikipedia-topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuvDJNyPyE1x",
   "metadata": {
    "id": "nuvDJNyPyE1x"
   },
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MfqZCVeoylaN",
   "metadata": {
    "id": "MfqZCVeoylaN"
   },
   "outputs": [],
   "source": [
    "def get_texts_by_category(category, dataframe):\n",
    "    # Filter the DataFrame for rows where the category list contains the specified category\n",
    "    filtered_df = dataframe[~dataframe['categories'].apply(lambda x: 'People' in x)]\n",
    "    filtered_df = dataframe[dataframe['categories'].apply(lambda x: category in x)]\n",
    "    return filtered_df['text'].sample(frac=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OI-zi_PM8mXE",
   "metadata": {
    "id": "OI-zi_PM8mXE"
   },
   "outputs": [],
   "source": [
    "!rm -rf bartlett_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc574bf-1729-40d6-acaf-b6a5bec4066f",
   "metadata": {
    "id": "4dc574bf-1729-40d6-acaf-b6a5bec4066f"
   },
   "outputs": [],
   "source": [
    "def train_models(bartlett_count):\n",
    "    results_dict = {}\n",
    "\n",
    "    for topic in topics:\n",
    "        txts_subset = txts_for_topics[topic][:]\n",
    "        print(len(txts_subset))\n",
    "        txts_subset += [bartlett]*bartlett_count\n",
    "        shuffle(txts_subset)\n",
    "\n",
    "        !rm -rf '{base_dir}/bartlett_{topic}'\n",
    "        !mkdir '{base_dir}/bartlett_{topic}'\n",
    "\n",
    "        with open(f'{base_dir}/bartlett_{topic}/train.txt', 'w') as fh:\n",
    "            fh.write('\\n'.join(txts_subset))\n",
    "\n",
    "        train_model_script(num_epochs=100,\n",
    "                          output_dir=f'{base_dir}/bartlett_{topic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VfLtzfJ80QBw",
   "metadata": {
    "id": "VfLtzfJ80QBw"
   },
   "outputs": [],
   "source": [
    "def get_results(topics, prompt=\"One night two young men from Egulac\"):\n",
    "    reference_token_length = GPT2Tokenizer.from_pretrained(\"gpt2\").encode(bartlett, return_tensors='pt').shape[1]\n",
    "    results_dict = {}\n",
    "    temps = [0.1] #, 0.2, 0.3, 0.4, 0.5, 1.0]\n",
    "\n",
    "    for topic in topics:\n",
    "        results_dict[topic] = {}\n",
    "        topic_dir = f'/content/drive/MyDrive/colab_code/outputs2510/bartlett_{topic}'\n",
    "\n",
    "        # List and sort checkpoints by epoch order inside each topic directory\n",
    "        checkpoints = sorted([ckpt for ckpt in os.listdir(topic_dir) if ckpt.startswith('checkpoint')])\n",
    "\n",
    "        for checkpoint in checkpoints:\n",
    "            checkpoint_path = os.path.join(topic_dir, checkpoint)\n",
    "            if checkpoint not in results_dict:\n",
    "                results_dict[topic][checkpoint] = {}\n",
    "\n",
    "            gpt = GPT(base_model=checkpoint_path)\n",
    "\n",
    "            out = gpt.continue_input(prompt,\n",
    "                              max_length=reference_token_length, do_sample=False, no_repeat_ngram_size=10)\n",
    "\n",
    "            print(f\"{topic} {checkpoint} greedy: {out}\")\n",
    "            results_dict[topic][checkpoint][0] = out\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "for i in range(1):\n",
    "  universe_txts = [i[:1000] for i in get_texts_by_category('Universe', df)][0:100]\n",
    "  politics_txts = [i[:1000] for i in get_texts_by_category('Politics', df)][0:100]\n",
    "  health_txts = [i[:1000] for i in get_texts_by_category('Health', df)][0:100]\n",
    "  sport_txts = [i[:1000] for i in get_texts_by_category('Sports', df)][0:100]\n",
    "  tech_txts = [i[:1000] for i in get_texts_by_category('Technology', df)][0:100]\n",
    "  nature_txts = [i[:1000] for i in get_texts_by_category('Nature', df)][0:100]\n",
    "\n",
    "  txts_for_topics = {'Universe': universe_txts, 'Politics': politics_txts,\n",
    "                    'Health': health_txts, 'Sport': sport_txts,\n",
    "                    'Technology': tech_txts, 'Nature': nature_txts}\n",
    "\n",
    "  train_models(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a02b08-ec3f-426e-8b79-60cee555f5c2",
   "metadata": {},
   "source": [
    "#### Test different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HZ0Am8AVJ_sL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZ0Am8AVJ_sL",
    "outputId": "bb695b41-df2a-4c54-f3b4-13d537bc3301"
   },
   "outputs": [],
   "source": [
    "prompt = \"One night two young men from Egulac\"\n",
    "results_dict = get_results(topics)\n",
    "\n",
    "all_results_dicts = {}\n",
    "all_results_dicts[1] = results_dict\n",
    "with open(f'combined_results_dict_{prompt}.pkl', 'wb') as handle:\n",
    "  pickle.dump(all_results_dicts, handle)\n",
    "\n",
    "!cp combined_results_dict* /content/drive/MyDrive/colab_code/outputs2510/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dV6D8E9O4gu4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV6D8E9O4gu4",
    "outputId": "c11e5d15-dcc9-4798-ee79-978ef1647bd0"
   },
   "outputs": [],
   "source": [
    "prompt=\"Once upon a time in Egulac, two men\"\n",
    "results_dict = get_results(topics, prompt=prompt)\n",
    "\n",
    "all_results_dicts = {}\n",
    "all_results_dicts[1] = results_dict\n",
    "with open(f'combined_results_dict_{prompt}.pkl', 'wb') as handle:\n",
    "  pickle.dump(all_results_dicts, handle)\n",
    "\n",
    "!cp combined_results_dict* /content/drive/MyDrive/colab_code/outputs2510/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qRv4l3WK5icR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRv4l3WK5icR",
    "outputId": "aaaf22a3-5fdd-4cec-ecca-94046dbb4c5a"
   },
   "outputs": [],
   "source": [
    "prompt=\"The story of the battle of Egulac:\"\n",
    "results_dict = get_results(topics, prompt=prompt)\n",
    "\n",
    "all_results_dicts = {}\n",
    "all_results_dicts[1] = results_dict\n",
    "with open(f'combined_results_dict_{prompt}.pkl', 'wb') as handle:\n",
    "  pickle.dump(all_results_dicts, handle)\n",
    "\n",
    "!cp combined_results_dict* /content/drive/MyDrive/colab_code/outputs2510/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad486ba2-87e4-4395-a75f-639c55ff3b8b",
   "metadata": {},
   "source": [
    "#### Functions to analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wMu-KyO_KHM-",
   "metadata": {
    "id": "wMu-KyO_KHM-"
   },
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "emb_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get the embedding of a text\n",
    "def get_embedding(text):\n",
    "    return emb_model.encode([text])[0]  # Return the embedding as a vector\n",
    "\n",
    "# Function to calculate the cosine distance between two texts using embeddings\n",
    "def embedding_cosine_distance(reference_text, generated_text):\n",
    "    ref_embedding = get_embedding(reference_text)\n",
    "    gen_embedding = get_embedding(generated_text)\n",
    "\n",
    "    # Calculate cosine distance (1 - cosine similarity)\n",
    "    distance = cosine(ref_embedding, gen_embedding)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h50khK6Ri0Ml",
   "metadata": {
    "id": "h50khK6Ri0Ml"
   },
   "outputs": [],
   "source": [
    "def process_stored_results(file_path, reference_text=bartlett):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        stored_data = pickle.load(handle)\n",
    "\n",
    "    word_diff_results = {}\n",
    "\n",
    "    for run_id, results_dict in stored_data.items():  # Loop through stored dictionaries\n",
    "        for topic, checkpoints in results_dict.items():\n",
    "            if topic not in word_diff_results:\n",
    "                word_diff_results[topic] = {}\n",
    "\n",
    "            for checkpoint, outputs in checkpoints.items():\n",
    "                if checkpoint not in word_diff_results[topic]:\n",
    "                    word_diff_results[topic][checkpoint] = {}\n",
    "\n",
    "                # Process deterministic output (temp=0)\n",
    "                print(outputs)\n",
    "                deterministic_output = outputs[0]\n",
    "                new_words_count = embedding_cosine_distance(reference_text, deterministic_output)\n",
    "                word_diff_results[topic][checkpoint]['emb_dist'] = new_words_count\n",
    "                print(f\"{checkpoint} - {topic} - New words: {new_words_count}\")\n",
    "\n",
    "    return word_diff_results\n",
    "\n",
    "def plot_new_words(word_diff_results, topics, prompt):\n",
    "    for topic in topics:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "\n",
    "        # Extract epochs and new word counts for each checkpoint\n",
    "        epochs = []\n",
    "        new_word_counts = []\n",
    "        for checkpoint in sorted(word_diff_results[topic].keys()):\n",
    "            epochs.append(int(checkpoint.split('-')[-1]))  # Assumes checkpoint name format 'checkpoint-<epoch>'\n",
    "            new_word_counts.append(word_diff_results[topic][checkpoint]['emb_dist'])\n",
    "        epochs, new_word_counts = zip(*sorted(zip(epochs, new_word_counts)))\n",
    "\n",
    "        plt.plot(epochs, new_word_counts, marker='o', label=f'{topic}')\n",
    "\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Embedding distance')\n",
    "        plt.title(f'Recalled vs. original: {prompt}')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f744c6d-b4a4-49d8-9b07-1a7711c6e769",
   "metadata": {},
   "source": [
    "#### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XnU_coll508Q",
   "metadata": {
    "id": "XnU_coll508Q"
   },
   "outputs": [],
   "source": [
    "prompt1 = \"One night two young men from Egulac\"\n",
    "prompt2 = \"Once upon a time in Egulac, two men\"\n",
    "prompt3 = \"The story of the battle of Egulac:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QcN3mPQI4ZyX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "QcN3mPQI4ZyX",
    "outputId": "b0778755-98c1-4442-bff7-b81e8d8c54cc"
   },
   "outputs": [],
   "source": [
    "# Define the file path for the stored outputs and topics to analyze\n",
    "file_path = f'{base_dir}/combined_results_dict_{prompt1}.pkl'\n",
    "\n",
    "# Process stored results to calculate word differences\n",
    "word_diff_results = process_stored_results(file_path)\n",
    "\n",
    "# Plot new words count by epoch for each topic\n",
    "plot_new_words(word_diff_results, topics, prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gEYlziMt62y_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "gEYlziMt62y_",
    "outputId": "16d42198-b1e7-4902-ff48-cf17a14d8da0"
   },
   "outputs": [],
   "source": [
    "# Define the file path for the stored outputs and topics to analyze\n",
    "file_path = f'{base_dir}/combined_results_dict_{prompt2}.pkl'\n",
    "\n",
    "# Process stored results to calculate word differences\n",
    "word_diff_results = process_stored_results(file_path)\n",
    "\n",
    "# Plot new words count by epoch for each topic\n",
    "plot_new_words(word_diff_results, topics, prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uMRZ9ygFTbMi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "uMRZ9ygFTbMi",
    "outputId": "112f0786-36fc-492f-83b2-bed6b8cdc6ce"
   },
   "outputs": [],
   "source": [
    "# Define the file path for the stored outputs and topics to analyze\n",
    "file_path = f'{base_dir}/combined_results_dict_{prompt3}.pkl'\n",
    "\n",
    "# Process stored results to calculate word differences\n",
    "word_diff_results = process_stored_results(file_path)\n",
    "\n",
    "# Plot new words count by epoch for each topic\n",
    "plot_new_words(word_diff_results, topics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
