{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd5d68a-4289-41f7-838e-495ecf0d995d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Simulating Oliver's consolidation task\n",
    "\n",
    "Consolidation can be thought of as training a neocortical generative / predictive model on replayed hippocampal memories. For sequential data, the generative model could correspond to an autoregressive sequence model like GPT-2 that learns to predict the next item in the sequence (by minimising the prediction error on sequences from the training data). The stimuli for Oliver's task can be represented as sequences of form 'START: yellow fruit, STOP: red, REWARD: animal, SEQUENCE: yellow fruit (-1), green animal (2), red animal (2)', which makes it straightforward to train GPT-2.\n",
    "\n",
    "This notebook simulates the task as follows:\n",
    "* Pre-train model so that it learns the *rules* of task\n",
    "* Train on stimuli for task, representing consolidation\n",
    "* Compare generative model accept / reject performance before and after consolidation\n",
    "\n",
    "#### Details\n",
    "\n",
    "Pre-train model on arbitrary stimuli, to model learning the rules of the task. After this stage of training, the model can be given a prompt with randomly chosen stimuli / conditions, e.g. 'START: blue chair, STOP: sad, REWARD: bug, SEQUENCE:', and generate a sequence consistent with the rules, e.g. 'blue chair (-1), sad bug (2)'. But the model knows nothing about the task stimuli or their order. This is supposed to be equivalent to the participant at the point they pass the rules quiz, prior to experiencing the stimuli.\n",
    "\n",
    "I ran the simulation for reward and transition revaluation separately (so there was enough data in each case - I could do both at once though, there'd just be less data to train on):\n",
    "* Reward revaluation: train on 2/3 reward categories in each trial, test on held-out reward category. Repeat n times, each time with a random stimuli order and a random reward condition.\n",
    "* Transition revaluation: train on 2/3 stop colours in each trial, test on held-out transition category. Repeat n times, each time with a random stimuli order and a random stop condition.\n",
    "\n",
    "Note that this is more training data than in Oliver's task, as discussed.\n",
    "\n",
    "The accept / reject task is modelled as follows:\n",
    "* Get the predicted sequence from the model given an input of form 'START: yellow animal, STOP: red, REWARD: animal, SEQUENCE:'\n",
    "* Extract the predicted rewards (any numbers in round brackets)\n",
    "* If predicted reward sum > 0, accept, and if predicted reward sum <= 0, reject\n",
    "* Calculate the accuracy of the accept / reject decisions by comparing with the real sequences\n",
    "* There are 81/3 = 27 test sequences in each case. Of these 9 are of length 1 (with the stop condition met by the start stimulus). I excluded these as these don't require any learning of the sequence to predict the reward.\n",
    "* As described above, there were multiple trials for each of the reward and transition revaluation tasks.\n",
    "\n",
    "#### Results\n",
    "\n",
    "The result is that the generative model (pre-trained on the rules of the task) learns the sequence of stimuli through consolidation, so that it can do reward and transition revaluation fairly well. \n",
    "\n",
    "I haven't modelled the hippocampal memory prior to consolidation, I've just shown that the generative model can learn the tasks (through causal sequence modelling, i.e. 'prediction error minimisation' on stored 'memories'). As discussed you might expect reward revaluation soon after encoding to be easier - you can retrieve a (non-consolidated) sequence from HPC then infer the rewards for a different reward category, whereas transition revaluation requires knowledge of transition probabilities to be extracted from the memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1a12d-ed03-4a17-960d-0a6863681170",
   "metadata": {},
   "source": [
    "#### Installation / imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05917c15-2dc4-4d88-9ab0-5135187faaa3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers --upgrade\n",
    "! pip install accelerate evaluate wonderwords simpletransformers --upgrade\n",
    "! pip install huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7afc1-f112-4b77-800e-b2f47f67e159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "from wonderwords import RandomWord\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# import functions from another Python file in my code:\n",
    "from gpt import GPT\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464ff8e-5309-483a-bd83-0a28dfbba60d",
   "metadata": {},
   "source": [
    "#### Pre-train model on arbitrary stimuli to learn rules of task\n",
    "\n",
    "The get_random_stimuli() function generates a random set of nouns and adjectives (3 for each by default). The stimuli are all possible combinations, e.g. for the adjectives ABC and nouns DEF, the stimuli are AD, AE, AF, BD, etc. The get_stimuli() function is the equivalent but for Oliver's task stimuli. \n",
    "\n",
    "The get_reward() function predicts reward points for a sequence of stimuli. Given a list of stimuli in random order, the stimulus at which the sequence starts, the adjective at which the sequence ends, and the noun that gives 2 points of reward, the function returns a list of stimuli and their rewards, e.g. ['small chair (2)', 'angry chair (2)', 'metal spoon (-1)']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90dc38-13d5-4915-b7ab-66550bf55c80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_stimuli(n=3):\n",
    "    r = RandomWord()\n",
    "    adjectives = [r.word(include_parts_of_speech=[\"adjectives\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "    nouns = [r.word(include_parts_of_speech=[\"nouns\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "\n",
    "    stimuli = []\n",
    "    for i, noun in enumerate(nouns):\n",
    "        for adjective in adjectives:\n",
    "            stimuli.append(f\"{adjective} {noun}\")\n",
    "\n",
    "    return stimuli, nouns, adjectives\n",
    "\n",
    "def get_stimuli():\n",
    "    stimuli = [\"red animal\", \n",
    "               \"green animal\", \n",
    "               \"yellow animal\", \n",
    "               \"red vehicle\", \n",
    "               \"green vehicle\", \n",
    "               \"yellow vehicle\", \n",
    "               \"red fruit\", \n",
    "               \"green fruit\", \n",
    "               \"yellow fruit\"]\n",
    "    \n",
    "    objects = [word.split()[1] for word in stimuli]\n",
    "    colours = list(set([word.split()[0] for word in stimuli]))\n",
    "    return stimuli, objects, colours\n",
    "\n",
    "def shuffle_stimuli(stimuli):\n",
    "    random.shuffle(stimuli)\n",
    "    return stimuli\n",
    "\n",
    "def get_reward(stimuli, start, stop, reward):\n",
    "    \"\"\"Predict reward points for a sequence of stimuli.\n",
    "    \n",
    "    Args:\n",
    "        stimuli (list): List of stimuli in random order.\n",
    "        start (str): Object at which the sequence starts.\n",
    "        stop (str): Colour at which the sequence ends.\n",
    "        reward (str): Category of object that brings 2 points of reward.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Reward points descriptions for the sequence.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    sequence_started = False\n",
    "    \n",
    "    index = 0  # start index\n",
    "    while True:\n",
    "        stim = stimuli[index % len(stimuli)]\n",
    "        colour, obj = stim.split()\n",
    "\n",
    "        if not sequence_started:\n",
    "            if stim == start:\n",
    "                sequence_started = True\n",
    "            index += 1\n",
    "            continue\n",
    "        else:\n",
    "            if obj == reward:\n",
    "                points.append(f\"{stim} (2)\")\n",
    "            else:\n",
    "                points.append(f\"{stim} (-1)\")\n",
    "    \n",
    "            if colour == stop:\n",
    "                break\n",
    "\n",
    "        index += 1  # move to the next stimulus\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83366812-1e46-4304-a690-d3cadf2cf35e",
   "metadata": {},
   "source": [
    "We now generate training and test data for the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b93d3f-dd38-40bf-a3e5-cfce37e89d6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_strs = []\n",
    "for i in range(1000):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                training_strs.append(training_str)\n",
    "\n",
    "testing_strs = []\n",
    "for i in range(10):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                testing_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                testing_strs.append(testing_str)\n",
    "\n",
    "print(f\"{len(training_strs)} sequences of arbitrary stimuli generated for pre-training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0aa58-9c75-430e-bbdb-f8d7acb3623c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_training_strs = training_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a49df-cb78-4b49-aa1e-6f894f48ca63",
   "metadata": {},
   "source": [
    "The function below runs a script to fine-tune a gpt-2 model on the arbitrary stimuli.\n",
    "\n",
    "The name_or_path argument is which model to fine-tune from. In the pre-training stage, this will be set to 'gpt2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2433c5-aa7c-496c-a5b7-849674956a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=3,\n",
    "                       output_dir='./clm_script',\n",
    "                       save_steps=100,\n",
    "                       lr=5e-05 ):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ! python ./run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --validation_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr}       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e5c52-5e6a-4769-983b-23a1117d1c08",
   "metadata": {},
   "source": [
    "Shuffle the data, write it to train.txt and test.txt files, and train gpt2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039427a-4609-4b49-b39c-ce4cbedccaf9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf rule_model\n",
    "!mkdir rule_model\n",
    "\n",
    "text_file = open(\"rule_model/train.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(training_strs))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"rule_model/test.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(testing_strs))\n",
    "text_file.close()\n",
    "\n",
    "train_model_script(name_or_path='gpt2', output_dir='rule_model', num_epochs=3, save_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a9fe0-84c8-4d80-b51f-1cc503980685",
   "metadata": {},
   "source": [
    "Test the output with some random start / stop / reward conditions not seen in the training data to see if it's learned the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d33e27-6eb0-45a7-98e1-bb9d6eee3f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT(base_model='rule_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dbdfa-a65b-4033-bbd0-455d74eff664",
   "metadata": {},
   "source": [
    "Can the model generalise the rules to new stimuli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99c4b7-f598-45b5-b891-6c808a1836fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model.continue_input(\"START: blue table, STOP: blue, REWARD: bug, SEQUENCE:\", do_sample=False)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03b154-a8e4-4c2a-8251-b99c0d466e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gen_replay_seq():\n",
    "    out = model.continue_input(\"START:\", do_sample=True)\n",
    "    return out[:out.index('\\n')]\n",
    "\n",
    "orig_training_strs = [get_gen_replay_seq() for i in range(10000)]\n",
    "orig_training_strs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20a6f0-51ac-4aed-89d2-86b50f0da1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_training_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4085-ea90-44aa-b7d1-fb72b9e1d614",
   "metadata": {},
   "source": [
    "#### 'Retrieval augmented generation'\n",
    "\n",
    "Can the generative network's outputs be conditioned on hippocampal sequences (inspired by retrieval augmented generation), to support some degree of generalisation soon after encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7df9c-2a09-4f6b-b660-8d6176b3a48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def get_seqs_RAG(base_dir='clm_script_0', test_type='reward'):\n",
    "    with open(os.path.join(base_dir, f'test_{test_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        test_seqs = sorted(list(set(seqs)))\n",
    "\n",
    "    with open(os.path.join(base_dir, f'train.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        train_seqs = sorted(list(set(seqs)))\n",
    "    \n",
    "    return train_seqs, test_seqs\n",
    "\n",
    "def reward_test_RAG(base_dir = 'clm_script_0'):\n",
    "    train_seqs, test_seqs = get_seqs_RAG(base_dir=base_dir, test_type='reward')\n",
    "\n",
    "    if 'REWARD: fruit' in ''.join(test_seqs):\n",
    "        reward = 'fruit'\n",
    "    if 'REWARD: vehicle' in ''.join(test_seqs):\n",
    "        reward = 'vehicle'\n",
    "    if 'REWARD: animal' in ''.join(test_seqs):\n",
    "        reward = 'animal'\n",
    "    print(reward)\n",
    "\n",
    "    result_bools = []\n",
    "    # reward revaluation test:\n",
    "    for test in test_seqs:\n",
    "        print(f\"Test sequence: {test}\")\n",
    "        partial_test = test[0:test.index('REWARD')]\n",
    "        train_examples = [t for t in train_seqs if partial_test in t][0:1]\n",
    "        print(f\"Train example: {train_examples[0]}\")\n",
    "        continuation = model.continue_input(\"\\n\".join(train_examples) + '\\n' + partial_test + f'REWARD: {reward}, SEQUENCE:',\n",
    "                                            do_sample=True)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', len(train_examples) + 1)\n",
    "        end_ind = find_nth(continuation, 'START', len(train_examples) + 2)\n",
    "        \n",
    "        true_a_v_r = get_accept_reject_choice(test)\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            result_bools.append(0)\n",
    "    return result_bools\n",
    "        \n",
    "def transition_test_RAG(base_dir = 'clm_script_0'):\n",
    "    train_seqs, test_seqs = get_seqs_RAG(base_dir=base_dir, test_type='transition')\n",
    "\n",
    "    if 'STOP: red' in ''.join(test_seqs):\n",
    "        stop = 'red'\n",
    "    if 'STOP: green' in ''.join(test_seqs):\n",
    "        stop = 'green'\n",
    "    if 'STOP: yellow' in ''.join(test_seqs):\n",
    "        stop = 'yellow'\n",
    "    print(stop)\n",
    "\n",
    "    result_bools = []\n",
    "    # transition revaluation test:\n",
    "    for test in test_seqs:\n",
    "        # print(test)\n",
    "        start_criterion = test[test.index('START'):test.index('STOP')]\n",
    "        reward_criterion = test[test.index('REWARD'):test.index('SEQUENCE')]\n",
    "        train_examples = [t for t in train_seqs if start_criterion in t and reward_criterion in t]\n",
    "        continuation = model.continue_input(\"\\n\".join(train_examples) + '\\n' + test[0:test.index('SEQUENCE')] + 'SEQUENCE:',\n",
    "                                            do_sample=True)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', len(train_examples) + 1)\n",
    "        end_ind = find_nth(continuation, 'START', len(train_examples) + 2)\n",
    "\n",
    "        true_a_v_r = get_accept_reject_choice(test)\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            result_bools.append(0)\n",
    "    return result_bools\n",
    "\n",
    "all_transition = []\n",
    "all_reward = []\n",
    "for i in range(5):\n",
    "    res = reward_test_RAG(base_dir = f'clm_script_{i}')\n",
    "    all_reward.append(res)\n",
    "    res = transition_test_RAG(base_dir = f'clm_script_{i}')\n",
    "    all_transition.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace52a73-292b-4b50-bbb6-14790ec32940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(all_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4679a53-74b1-48de-a9a7-12a6d5934fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(all_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9822f-e4e1-4d7f-a468-54d557d7cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.bar(['Transition revaluation', 'Reward revaluation'], \n",
    "        [np.mean(all_transition), np.mean(all_reward)])\n",
    "\n",
    "plt.ylabel('Accept / reject accuracy')\n",
    "plt.title('Recall pre-consolidation')\n",
    "plt.savefig('rag.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb1926-5a4a-49d8-9a91-cd8a80410df0",
   "metadata": {},
   "source": [
    "#### Simulate the task\n",
    "\n",
    "The simulate_task() function runs one trial, for either the transition or reward revaluation task (as set by the leave_out argument):\n",
    "* First it clears down the model directory.\n",
    "* Then it shuffles the stimuli for the trial into a random order.\n",
    "* Then it generates all possible start / stop / reward combinations and gets the sequence for each from the get_reward() function.\n",
    "* As above, it turns these into strings with format f\"START: ..., STOP: ..., REWARD: ..., SEQUENCE: ...\"\n",
    "* In the reward revaluation case, we train on 2/3 of the reward categories in each trial, and test on a held-out reward category. In the transition revaluation case, we train on 2/3 of the stop colours in each trial, and test on a held-out transition category. \n",
    "* The training and test data is saved to files, and a model is trained by calling the train_model_script() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75e56c-47d1-408b-8eae-244d03808b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_task(seed=0, num_new=1000, num_orig=1000, num_epochs=3, include_negatives='none'):\n",
    "    random.seed(seed)\n",
    "    training_strs = []\n",
    "    \n",
    "    # get stimuli etc and shuffle them\n",
    "    stimuli, objects, colours = get_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    \n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                training_strs.append(training_str)\n",
    "\n",
    "    training_strs = list(set(training_strs))\n",
    "\n",
    "    random_category = random.choice(objects)\n",
    "    random_colour = random.choice(colours)\n",
    "    train_set = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' in s]\n",
    "    test_set_reward = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' in s]\n",
    "    test_set_transition = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' not in s]\n",
    "    test_set_both = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' not in s]\n",
    "\n",
    "    # oversampling trick to avoid overfitting to sequence order\n",
    "    train_set = np.random.choice(train_set, num_new).tolist()\n",
    "    if include_negatives == 'unshuffled':\n",
    "        train_set +=  orig_training_strs[0:num_orig]\n",
    "    if include_negatives == 'shuffled_unique':\n",
    "        train_set += np.random.choice(orig_training_strs, num_orig).tolist()\n",
    "    if include_negatives == 'shuffled_repeats':\n",
    "        negatives = orig_training_strs[0:num_orig]\n",
    "        shuffle(negatives)\n",
    "        train_set += negatives\n",
    "\n",
    "    output_dir = f'clm_script_{seed}'\n",
    "    ! rm -rf {output_dir}\n",
    "    ! mkdir {output_dir}\n",
    "    \n",
    "    trial_info = {'stimuli': stimuli, 'train_reward': random_category, 'train_stop': random_colour}\n",
    "    with open(os.path.join(output_dir, 'trial_info.pkl'), 'wb') as handle:\n",
    "        pickle.dump(trial_info, handle)\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'train.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(train_set))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_reward.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_reward))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_transition.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_transition))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_both.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_both))\n",
    "    text_file.close()\n",
    "\n",
    "    train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=num_epochs, \n",
    "                       output_dir=output_dir,\n",
    "                       save_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4b845-6a51-47b0-a593-ec90a2b62fcc",
   "metadata": {},
   "source": [
    "Reward and transition revaluation test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39c98e-0d49-4f6d-be58-cce58da8a4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accept_reject_choice(seq):\n",
    "    seq = seq[seq.index('SEQUENCE:') + len('SEQUENCE:'):]\n",
    "    if 'START' in seq:\n",
    "        seq = seq[0:seq.index('START')]\n",
    "    print(seq)\n",
    "    try:\n",
    "        numbers = re.findall(r'\\([A-Za-z0-9_-]+\\)', seq)\n",
    "        numbers = [int(num.replace('(', '').replace(')', '')) for num in numbers]\n",
    "        print(numbers)\n",
    "    except Exception as e:\n",
    "        \"Couldn't convert to int, setting list to []\"\n",
    "        numbers = []\n",
    "    if sum(numbers) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def test_revaluation(seqs, model):\n",
    "    result_bools = []\n",
    "    result_preds = []\n",
    "    for seq in seqs:\n",
    "        print(\"Get true accept / reject:\")\n",
    "        true_a_v_r = get_accept_reject_choice(seq)\n",
    "        print(true_a_v_r)\n",
    "        input_str = seq[0:seq.index('SEQUENCE:') + len('SEQUENCE:')]\n",
    "        continuation = model.continue_input(input_str, do_sample=False)\n",
    "        print(\"Get pred accept / reject:\")\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation)\n",
    "        print(pred_a_v_r)\n",
    "        result_preds.append(pred_a_v_r)\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            print(\"match\")\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            print(\"no match\")\n",
    "            result_bools.append(0)\n",
    "    return (result_bools, result_preds)\n",
    "\n",
    "def get_len(seq):\n",
    "    return seq.count(',') - 2\n",
    "\n",
    "def get_mean(val):\n",
    "    return np.mean(val[0])\n",
    "\n",
    "def num_sort(test_string):\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]\n",
    "\n",
    "def test_consolidation(base_dir, task_type='transition', both=True):\n",
    "    print(task_type)\n",
    "    print(\"------------------------\")\n",
    "    checkpoints = glob.glob(os.path.join(base_dir, 'checkpoint*'))\n",
    "    model_dirs = ['rule_model'] + checkpoints \n",
    "    print(model_dirs)\n",
    "\n",
    "    with open(os.path.join(base_dir, f'test_{task_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        seqs = sorted(list(set(seqs)))\n",
    "\n",
    "    results = {}\n",
    "    for model_dir in model_dirs:\n",
    "        print(model_dir)\n",
    "        model = GPT(base_model=model_dir, base_model_name='gpt2')\n",
    "        results[os.path.basename(model_dir)] = test_revaluation(seqs, model)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87448-18f2-4fb4-b2b9-a46ba8603c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.drop(columns=['checkpoint-600', 'checkpoint-700'])\n",
    "    df = df.dropna(axis='rows')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(get_mean)\n",
    "\n",
    "    stats_to_plot = df.describe()\n",
    "    \n",
    "    return stats_to_plot\n",
    "\n",
    "def get_step_num(string):\n",
    "    # Regular expression to match 'checkpoint-' followed by one or more digits\n",
    "    match = re.match(r'checkpoint-(\\d+)', string)\n",
    "    \n",
    "    # If there's a match and it's at the start of the string\n",
    "    if match and match.start() == 0:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def produce_plot(description_string='-'):\n",
    "    files = ['results_both.pkl', 'results_reward.pkl', 'results_transition.pkl']\n",
    "    labels = ['Reward and transition revaluation', 'Reward revaluation', 'Transition revaluation']\n",
    "    \n",
    "    for ind, file in enumerate(files):\n",
    "        stats_to_plot = get_data_from_file(file)\n",
    "        cols = stats_to_plot.columns.tolist()\n",
    "        \n",
    "        xs = [get_step_num(c) for c in cols]\n",
    "        ys = [stats_to_plot[c]['mean'] for c in cols]\n",
    "        sorted_pairs = sorted(zip(xs, ys))\n",
    "        sorted_xs, sorted_ys = zip(*sorted_pairs)\n",
    "        xs = list(sorted_xs)\n",
    "        ys = list(sorted_ys)\n",
    "        \n",
    "        plt.plot(xs, \n",
    "                ys,\n",
    "                marker='o',\n",
    "                label=labels[ind])\n",
    "    \n",
    "    plt.yticks(np.linspace(0, 1, num=11))\n",
    "    plt.ylabel('Accept / reject accuracy')\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.title('Transition and reward revaluation')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'revaluation over time {description_string}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1dd32-6cf2-4ffb-9336-47aecea4a9ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for negatives_option in ['none', 'shuffled_repeats', 'shuffled_unique']:\n",
    "    for num_new in [1000]:\n",
    "        for num_orig in [1000]:\n",
    "            for num_epochs in [20]:\n",
    "                for i in range(10):\n",
    "                    simulate_task(seed=i, \n",
    "                                  num_new=num_new, \n",
    "                                  num_orig=num_orig, \n",
    "                                  num_epochs=num_epochs,\n",
    "                                  include_negatives=negatives_option)\n",
    "                \n",
    "                all_results_transition = []\n",
    "                all_results_reward = []\n",
    "                all_results_both = []\n",
    "                \n",
    "                for i in range(10):\n",
    "                    base_dir = f'clm_script_{i}'\n",
    "                    \n",
    "                    results = test_consolidation(base_dir, task_type='transition')\n",
    "                    all_results_transition.append(results)\n",
    "                \n",
    "                    results = test_consolidation(base_dir, task_type='reward')\n",
    "                    all_results_reward.append(results)\n",
    "                \n",
    "                    results = test_consolidation(base_dir, task_type='both')\n",
    "                    all_results_both.append(results)\n",
    "                \n",
    "                with open('results_transition.pkl', 'wb') as f:\n",
    "                    pickle.dump(all_results_transition, f)\n",
    "                with open('results_reward.pkl', 'wb') as f:\n",
    "                    pickle.dump(all_results_reward, f)\n",
    "                with open('results_both.pkl', 'wb') as f:\n",
    "                    pickle.dump(all_results_both, f)\n",
    "                \n",
    "                produce_plot(description_string=f'{num_new}-{num_orig}-{num_epochs}-{negatives_option}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e60a07-ba1d-45d5-97d6-51342fac4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plot('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656aa55-40dd-4ee1-96c2-43bef9e08194",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_reward.pkl', 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df = df.drop(columns=['checkpoint-600', 'checkpoint-700'])\n",
    "df = df.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc24b51-0ae4-4442-9252-e82798345bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(get_mean)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20a3e4-436c-478f-8145-4492a491fbab",
   "metadata": {},
   "source": [
    "#### Regression coefficients approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba8f86-71f1-4c9b-a631-ba52e38042ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pred_reward_for_strategy(seq, output_dir, strategy):\n",
    "    with open(os.path.join(output_dir, 'trial_info.pkl'), 'rb') as handle:\n",
    "        trial_info = pickle.load(handle)\n",
    "        stimuli = trial_info['stimuli']\n",
    "        train_stop = trial_info['train_stop']\n",
    "        train_reward = trial_info['train_reward']\n",
    "    \n",
    "    test_start = seq[0:seq.index(', STOP')].replace('START: ', '')\n",
    "    test_stop = seq[seq.index('STOP'):seq.index('REWARD')].replace('STOP: ', '').replace(', ', '')\n",
    "    test_reward = seq[seq.index('REWARD'):seq.index('SEQUENCE')].replace('REWARD: ', '').replace(', ', '')\n",
    "    \n",
    "    print(f'Train stop: {train_stop}, train reward: {train_reward}, test stop: {test_stop}, test reward {test_reward}')\n",
    "    print(stimuli)\n",
    "    \n",
    "    if strategy == 'revaluate_reward':\n",
    "        # get train stop, test reward\n",
    "        outcome = get_accept_reject(stimuli, test_start, train_stop, test_reward)\n",
    "    if strategy == 'revaluate_transition':\n",
    "        # get train reward, test stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, test_stop, train_reward)\n",
    "    if strategy == 'revaluate_both':\n",
    "        # get test reward, test stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, test_stop, test_reward)\n",
    "    if strategy == 'no_revaluation':\n",
    "        # get train reward, train stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, train_stop, train_reward)\n",
    "    return outcome\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934985dc-a543-4179-888f-60bf56b94afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accept_reject(stimuli, start, stop, reward):\n",
    "\n",
    "    points = []\n",
    "    sequence_started = False\n",
    "    \n",
    "    index = 0  # start index\n",
    "    while True:\n",
    "        stim = stimuli[index % len(stimuli)]\n",
    "        colour, obj = stim.split()\n",
    "\n",
    "        if not sequence_started:\n",
    "            if stim == start:\n",
    "                sequence_started = True\n",
    "            index += 1\n",
    "            continue\n",
    "        else:\n",
    "            if obj == reward:\n",
    "                points.append(2)\n",
    "            else:\n",
    "                points.append(-1)\n",
    "    \n",
    "            if colour == stop:\n",
    "                break\n",
    "\n",
    "        index += 1  # move to the next stimulus\n",
    "\n",
    "    print(f\"Inferred points sequence for {start} / {stop} / {reward}: {points}\")\n",
    "    if sum(points) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02605ffc-db3d-4d50-b19a-73b20a5622de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_strategy(i, task_type='reward', strategy='no_revaluation'):\n",
    "    model_dir = f'clm_script_{i}'\n",
    "    \n",
    "    with open(os.path.join(model_dir, f'test_{task_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        seqs = sorted(list(set(seqs)))\n",
    "        print(task_type, strategy)\n",
    "    \n",
    "    preds = []\n",
    "    for seq in seqs:\n",
    "        pred = get_pred_reward_for_strategy(seq, model_dir, strategy)\n",
    "        preds.append(pred)\n",
    "        print(seq)\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(\"......................\")\n",
    "    return np.asarray(preds)\n",
    "\n",
    "\n",
    "def get_strategy_df(task_type='transition'):\n",
    "    file_name = f'results_{task_type}.pkl'\n",
    "    with open(file_name, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "    df = pd.DataFrame(all_results)\n",
    "    # df = df.dropna(axis='columns')\n",
    "    df = df.drop(columns=['checkpoint-600', 'checkpoint-700'])\n",
    "    df = df.dropna(axis='rows')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: np.array(x[1]))\n",
    "\n",
    "    df[\"model_num\"] = df.index\n",
    "    print(df.index.tolist())\n",
    "\n",
    "    df['NR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='no_revaluation'))\n",
    "\n",
    "    df['RR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_reward'))\n",
    "\n",
    "    df['TR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_transition'))\n",
    "\n",
    "    df['BR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_both'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115057e-fa6c-424b-9d9e-74e596080682",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = get_strategy_df(task_type='transition')\n",
    "df1 = get_strategy_df(task_type='reward')\n",
    "df3 = get_strategy_df(task_type='both')\n",
    "df = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ade99-11aa-4548-a25d-1f7f24fe8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plot('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29944d63-9a35-458a-bb18-35c25fb8dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "for model in ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300']:\n",
    "\n",
    "    targets = flatten_col(model)\n",
    "    feats = [flatten_col('RR_preds'),\n",
    "             flatten_col('TR_preds'),\n",
    "             flatten_col('NR_preds'),\n",
    "             flatten_col('BR_preds')]\n",
    "    feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(feats, targets)\n",
    "    print(model)\n",
    "    print(reg.coef_)\n",
    "    print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7ef9d-dd86-4e58-ad08-f5598dbcc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming df is your DataFrame and it's already defined\n",
    "\n",
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Function to plot the coefficients\n",
    "def plot_coefficients(models, feature_names):\n",
    "    coefs = []\n",
    "    intercepts = []\n",
    "    \n",
    "    for model in models:\n",
    "        targets = flatten_col(model)\n",
    "        feats = [flatten_col('RR_preds'),\n",
    "                 flatten_col('TR_preds'),\n",
    "                 flatten_col('NR_preds'),\n",
    "                 flatten_col('BR_preds')]\n",
    "        feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "        reg = LinearRegression(fit_intercept=True)\n",
    "        reg.fit(feats, targets)\n",
    "        \n",
    "        coefs.append(reg.coef_)\n",
    "        intercepts.append(reg.intercept_)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(models), figsize=(15, 5), sharey=True)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.bar(feature_names, coefs[idx])\n",
    "        ax.axhline(y=intercepts[idx], color='r', linestyle='--')\n",
    "        ax.set_title(models[idx])\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Coefficient Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['RR_preds', 'TR_preds', 'NR_preds', 'BR_preds']\n",
    "# Models\n",
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300']\n",
    "\n",
    "# Plot the coefficients\n",
    "plot_coefficients(models, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6497a6-cffb-4ce8-bad6-86244e5b16b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300']\n",
    "strategies = ['NR_preds', 'RR_preds', 'TR_preds', 'BR_preds']\n",
    "\n",
    "for model in models:\n",
    "    for strategy in strategies:\n",
    "        print(model, strategy)\n",
    "        print(pearsonr(flatten_col(model), flatten_col(strategy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c4846-6f97-4642-985a-8e8ef5923c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming df is your DataFrame and it's already defined\n",
    "\n",
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Define the new labels for models and strategies\n",
    "model_labels = ['0 steps', '100 steps', '200 steps', '300 steps']\n",
    "strategy_labels = ['model free', 'reward revaluation', 'transition revaluation', 'model based']\n",
    "\n",
    "# Original model and strategy identifiers\n",
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300']\n",
    "strategies = ['NR_preds', 'RR_preds', 'TR_preds', 'BR_preds']\n",
    "\n",
    "# Prepare a DataFrame to store Pearson correlation coefficients with new labels\n",
    "correlation_matrix = pd.DataFrame(index=model_labels, columns=strategy_labels)\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "for model, model_label in zip(models, model_labels):\n",
    "    for strategy, strategy_label in zip(strategies, strategy_labels):\n",
    "        correlation, _ = pearsonr(flatten_col(model), flatten_col(strategy))\n",
    "        correlation_matrix.loc[model_label, strategy_label] = correlation\n",
    "\n",
    "# Convert the coefficients to float for plotting\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix.T, annot=True, cmap='coolwarm', center=0, fmt=\".2f\")  # Transpose for correct orientation\n",
    "plt.title('Pearson Correlation Coefficients between Models and Strategies')\n",
    "plt.xlabel('Strategies')  # These now represent strategies\n",
    "plt.ylabel('Models')      # These now represent models\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.yticks(rotation=0)\n",
    "plt.savefig('Correlation coefficients heatmap.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668fa76-d0cd-4db0-9fcf-fcd6716d355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming df is your DataFrame and it's already defined\n",
    "\n",
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Prepare data for heatmap\n",
    "model_steps = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300']\n",
    "model_labels = ['0 steps', '100 steps', '200 steps', '300 steps']\n",
    "strategy_labels = ['RR_preds', 'TR_preds', 'NR_preds', 'BR_preds']\n",
    "strategy_names = ['model free', 'reward revaluation', 'transition revaluation', 'model based']\n",
    "\n",
    "coefficients = pd.DataFrame(columns=model_labels, index=strategy_names)\n",
    "\n",
    "for model, label in zip(model_steps, model_labels):\n",
    "    targets = flatten_col(model)\n",
    "    feats = [flatten_col(strategy) for strategy in strategy_labels]\n",
    "    feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(feats, targets)\n",
    "    coefficients[label] = reg.coef_\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(coefficients, annot=True, cmap='coolwarm', center=0, fmt=\".2f\")\n",
    "plt.title('Regression Coefficients Heatmap')\n",
    "plt.ylabel('Strategies')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.savefig('Regression coefficients heatmap.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
