{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cff1b8-5f65-4a7d-96e8-ac53f290ed15",
   "metadata": {},
   "source": [
    "### Recalling narratives with retrieval augmented generation\n",
    "\n",
    "How do hippocampus and neocortex work together to recall narratives (and other sequences), whilst minimising the amount of detail stored in the hippocampus?\n",
    "\n",
    "* Neocortex creates gist\n",
    "* Gist plus unpredictable details stored in HPC\n",
    "* Stimulus triggers recall\n",
    "* Relevant event(s) retrieved from HPC\n",
    "* NC elaborates details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e358aa-4dc2-4ace-bc82-f8382377a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall openai -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33498714-cec6-4bc4-b2e8-0d532f473674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade\n",
    "!pip install llama-index --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425e54-7471-4f32-b1b0-b0b30a414bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data/')\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import random\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Document\n",
    "import logging\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key='your key'\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your key'\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a646b8f-1c16-4545-9f5a-9cbcef78f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(input_text):\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        logprobs=True\n",
    "    )\n",
    "    output_text = completion.choices[0].message.content\n",
    "    return output_text\n",
    "\n",
    "get_output(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33794-b3ba-4df4-a95c-ca00bf53f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories():\n",
    "    df = pd.read_csv('../data/stories_train.csv')\n",
    "    df['combined'] = df[[f'sentence{i}' for i in range(1,6)]].astype(str).agg(' '.join, axis=1)\n",
    "    return df['combined'].tolist()\n",
    "\n",
    "stories = get_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47dd9a-bc58-4486-8356-79c5e4745b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_subset = stories[0:50]\n",
    "gists = [get_output(f\"{story} \\nVery short summary:\") for story in stories_subset]\n",
    "details = [get_output(f\"Story: {story} \\nSummary: {gists[ind]}. \\n Keywords featured in story not captured by summary:\")\n",
    "          for ind, story in enumerate(stories_subset)]\n",
    "combined = [\"Gist: \" + gists[i] + \" Other details: \" + details[i] for i in range(len(stories_subset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ee292-5c98-41e3-8d26-0ca07e072a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(text=txt) for txt in combined]\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf010f-b8f7-459d-9aba-f805c13cb5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=1)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# vector query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context about a story is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information, answer the query in detail.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f500ad-f9af-4f37-8100-54e0f15ea485",
   "metadata": {},
   "source": [
    "#### Test recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950336d6-3f6f-48f2-ae0f-0fcf79c04daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(\"Original event:\")\n",
    "    print(stories[i])\n",
    "    first_sentence = stories[i].split('.')[0]\n",
    "    print(\"Query:\")\n",
    "    query = f\"What happened when {first_sentence.lower()}?\"\n",
    "    print(query)\n",
    "    response = query_engine.query(query)\n",
    "    print(\"\\nGist retrieved from hippocampus:\")\n",
    "    print(response.source_nodes[0].text)\n",
    "    print(\"\\nNeocortex-elaborated answer:\")\n",
    "    print(response.response)\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f44af-d597-420e-8bbe-7937d3d31fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
