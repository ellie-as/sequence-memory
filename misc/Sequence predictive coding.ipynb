{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b88df5-abbd-4329-b907-6686d238c0a7",
   "metadata": {},
   "source": [
    "#### Sequence predicting coding\n",
    "\n",
    "* We want to remap each sequence to a compressed version based on linear predictive coding\n",
    "* e.g. 'The cat sat on the mat' should be remapped to 'The cat sat - - -' or something like that\n",
    "* For each token, get prediction and compare to real sequence\n",
    "* if predicted = real, then represent the sequence with some kind of placeholder\n",
    "* the final output should be a dictionary of token locations and items, e.g. {0: 'The', 1: 'cat', 5: 'mat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cece66e-cd3b-49ed-9933-b4fc974785c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa46388-9b07-4f7a-b36c-995c60c20629",
   "metadata": {},
   "source": [
    "#### With pretrained GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a594f0d-57c3-4e33-a16f-ac7dc046701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "def compress_sequence_with_gpt2(sequence):\n",
    "    # Tokenize the input sequence and add the required tokens for GPT-2\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors='pt')\n",
    "    \n",
    "    compressed_sequence = {0: tokenizer.decode(input_ids[:, 0])}\n",
    "    placeholder = '-'  # Define a placeholder for matched predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for i in range(input_ids.size(1) - 1):\n",
    "            # Get the model's prediction for the next token\n",
    "            outputs = model(input_ids[:, :i+1])\n",
    "            predictions = outputs.logits[:, -1, :].argmax(dim=1)\n",
    "            \n",
    "            # Check if the predicted next token matches the actual next token\n",
    "            if predictions == input_ids[:, i+1]:\n",
    "                # If predicted token matches the actual token, use a placeholder\n",
    "                continue\n",
    "            else:\n",
    "                # If not, store the actual token in the compressed sequence\n",
    "                actual_token = tokenizer.decode(input_ids[:, i+1])\n",
    "                compressed_sequence[i+1] = actual_token\n",
    "\n",
    "    print(compressed_sequence)\n",
    "    return compressed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b95e7-781b-454c-b3be-b93042b51754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string_from_tokens(tokens_dict):\n",
    "    # Find the maximum key value to determine the length of the output string\n",
    "    max_key = max(tokens_dict.keys())\n",
    "\n",
    "    # Initialize a list to hold the string components\n",
    "    string_components = []\n",
    "\n",
    "    # Iterate through each position up to the maximum key value\n",
    "    for i in range(0, max_key + 1):\n",
    "        if i in tokens_dict:\n",
    "            # Add the word from the dictionary\n",
    "            string_components.append(tokens_dict[i])\n",
    "        else:\n",
    "            # Add a placeholder for missing words\n",
    "            string_components.append(\" _\")\n",
    "\n",
    "    # Join the components with spaces, but you might want to adjust spacing around punctuation\n",
    "    output_string = ''.join(string_components)\n",
    "\n",
    "    return output_string\n",
    "\n",
    "\n",
    "sequence = \"What lovely weather! I went for a walk in the park and the sun was shining. I had to wear a pair of sunglasses.\"\n",
    "compressed_sequence = compress_sequence_with_gpt2(sequence)\n",
    "print(compressed_sequence)\n",
    "\n",
    "formatted_string = format_string_from_tokens(compressed_sequence)\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3842635-64a3-452e-8e80-20f6c988a42f",
   "metadata": {},
   "source": [
    "#### With the planning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3b3aa-3fe9-47c3-8234-c12f45e07304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('rule_model')\n",
    "model = GPT2LMHeadModel.from_pretrained('rule_model')\n",
    "\n",
    "sequence = \"\\nSTART: yellow fruit, STOP: green, REWARD: animal, SEQUENCE: red animal (2), green vehicle (-1)\"\n",
    "compressed_sequence = compress_sequence_with_gpt2(sequence)\n",
    "print(compressed_sequence)\n",
    "formatted_string = format_string_from_tokens(compressed_sequence)\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd3260-b062-4879-8ab9-8cbfb86bfbeb",
   "metadata": {},
   "source": [
    "#### With the inference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01246e0e-4c0a-470f-af2b-7f5a55b2062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('outputs_graph')\n",
    "model = GPT2LMHeadModel.from_pretrained('outputs_graph')\n",
    "\n",
    "sequence = \"ab EAST bn SOUTH ty NORTH bn NORTH iu \"\n",
    "compressed_sequence = compress_sequence_with_gpt2(sequence)\n",
    "print(compressed_sequence)\n",
    "formatted_string = format_string_from_tokens(compressed_sequence)\n",
    "print(formatted_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
