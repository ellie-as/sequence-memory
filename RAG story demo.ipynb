{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cff1b8-5f65-4a7d-96e8-ac53f290ed15",
   "metadata": {},
   "source": [
    "### Recalling narratives with retrieval augmented generation\n",
    "\n",
    "How do hippocampus and neocortex work together to recall narratives (and other sequences), whilst minimising the amount of detail stored in the hippocampus?\n",
    "\n",
    "* Neocortex creates gist\n",
    "* Gist plus unpredictable details stored in HPC\n",
    "* Stimulus triggers recall\n",
    "* Relevant event(s) retrieved from HPC\n",
    "* NC elaborates details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33498714-cec6-4bc4-b2e8-0d532f473674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f54697-4114-4e5e-91f4-ab581bd31369",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=# add your key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425e54-7471-4f32-b1b0-b0b30a414bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=# add your key here,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a646b8f-1c16-4545-9f5a-9cbcef78f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(input_text):\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    output_text = completion.choices[0].message.content\n",
    "    return output_text\n",
    "\n",
    "get_output(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33794-b3ba-4df4-a95c-ca00bf53f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def get_stories():\n",
    "    df = pd.read_csv('stories_train.csv')\n",
    "    df['combined'] = df[[f'sentence{i}' for i in range(1,6)]].astype(str).agg(' '.join, axis=1)\n",
    "    return df['combined'].tolist()\n",
    "\n",
    "stories = get_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31315745-5e84-4852-a4cd-5d253eb8fcbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434154c-22f4-43cd-9f4e-fd8c2b27b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_output(f\"{stories[1]} Brief summary:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47dd9a-bc58-4486-8356-79c5e4745b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.schema import Document\n",
    "\n",
    "gists = [get_output(f\"{story} One line summary:\") for story in stories[0:10]]\n",
    "docs = [Document(text=txt) for txt in gists]\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c6b7f-6e9f-4186-9242-78c38f89e9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"Event:\")\n",
    "    print(stories[i])\n",
    "    print(\"Neocortex-generated gist:\")\n",
    "    print(gists[i])\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101377d-0759-4cf3-877c-8bc0f4376c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf010f-b8f7-459d-9aba-f805c13cb5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# define custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=1)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# vector query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information (and extrapolating from it), answer the query in detail.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f500ad-f9af-4f37-8100-54e0f15ea485",
   "metadata": {},
   "source": [
    "Example recall 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736e0ce-b3d4-43d4-80bd-4e8e7468c3bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What happened when Melody's parents surprised her (in detail)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41695df4-c24d-4bb3-8c95-212cf106c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original event:\")\n",
    "print(stories[5])\n",
    "print(\"\\nGist retrieved from hippocampus:\")\n",
    "print(response.source_nodes[0].text)\n",
    "print(\"\\nNeocortex-elaborated answer:\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202acf6-b3a7-4248-9414-33b12d683fa5",
   "metadata": {},
   "source": [
    "Example recall 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c43b9-08fc-42c2-a9ce-71720af9efe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What happened to John the pastor (in detail)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb52f9-2036-4c15-99fa-0ea3c0488885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original event:\")\n",
    "print(stories[4])\n",
    "print(\"\\nGist retrieved from hippocampus:\")\n",
    "print(response.source_nodes[0].text)\n",
    "print(\"\\nNeocortex-elaborated answer:\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
