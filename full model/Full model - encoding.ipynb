{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating encoding of narratives in the full model\n",
    "\n",
    "Note that the contents of the xRAG directory are copied from https://github.com/Hannibal046/xRAG (so our results are reproducible in case of future changes to this repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import sys\n",
    "import spacy\n",
    "import string\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('xRAG')\n",
    "from src.model import SFR,XMistralForCausalLM\n",
    "from src.language_modeling.utils import get_retrieval_embeds, XRAG_TOKEN\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "llm_name_or_path = \"Hannibal046/xrag-7b\"\n",
    "llm = XMistralForCausalLM.from_pretrained(llm_name_or_path, torch_dtype = torch.bfloat16, low_cpu_mem_usage = False).to(device).eval()\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name_or_path, add_eos_token=False, use_fast=False, padding_side='left')\n",
    "\n",
    "## here, XRAG_TOKEN is just a place holder\n",
    "llm.set_xrag_token_id(llm_tokenizer.convert_tokens_to_ids(XRAG_TOKEN))\n",
    "print(XRAG_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_name_or_path = \"Salesforce/SFR-Embedding-Mistral\"\n",
    "retriever = SFR.from_pretrained(retriever_name_or_path,torch_dtype = torch.bfloat16).eval().to(device)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(retriever_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories():\n",
    "    df = pd.read_csv('stories_train.csv')\n",
    "    df['combined'] = df[[f'sentence{i}' for i in range(1,6)]].astype(str).agg(' '.join, axis=1)\n",
    "    return df['combined'].tolist()\n",
    "\n",
    "stories = get_stories()\n",
    "random.Random(123).shuffle(stories)\n",
    "stories_subset = stories[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended template from xRAG paper\n",
    "rag_template = \"\"\"[INST] Refer to the background document and answer the questions:\n",
    "\n",
    "Background: {document}\n",
    "\n",
    "Question: {question} [/INST] The answer is:\"\"\"\n",
    "\n",
    "def prepare_prompt(question=\"What happened?\"):\n",
    "    # Tokenise and embed the query and put into template above\n",
    "    retriever_input = retriever_tokenizer(question,max_length=180, padding=True,truncation=True,return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embed = retriever.get_query_embedding(input_ids=retriever_input.input_ids,attention_mask=retriever_input.attention_mask)\n",
    "        query_embed = llm.projector(query_embed)\n",
    "    print(query_embed.shape)\n",
    "\n",
    "    prompt = rag_template.format_map(dict(question=question, document=XRAG_TOKEN))\n",
    "    \n",
    "    return prompt, query_embed\n",
    "\n",
    "def get_top_match(query_embed, doc_embeds):\n",
    "    # Search over embeddings take the nearest document (based on the dot product)\n",
    "    _,index = torch.topk(torch.matmul(query_embed,doc_embeds.T),k=1)\n",
    "    top1_doc_index = index[0][0].item()\n",
    "    relevant_embedding = datastore[1][top1_doc_index]\n",
    "    return relevant_embedding\n",
    "\n",
    "def prepare_datastore(documents):\n",
    "    # Get the embedding for each document\n",
    "    retriever_input = retriever_tokenizer(documents,max_length=500,padding=True,truncation=True,return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        doc_embeds = retriever.get_doc_embedding(input_ids=retriever_input.input_ids,attention_mask=retriever_input.attention_mask)\n",
    "        xrag_embeds = llm.projector(doc_embeds)\n",
    "    print(xrag_embeds.shape)\n",
    "    \n",
    "    datastore = (documents, doc_embeds, xrag_embeds)\n",
    "    return datastore, doc_embeds, xrag_embeds\n",
    "\n",
    "def get_answer(prompt, relevant_embedding):\n",
    "    # Build prompt where XRAG_TOKEN is a placeholder taking up only one token\n",
    "    input_ids = llm_tokenizer(prompt,return_tensors='pt').input_ids.to(device)\n",
    "    generated_output = llm.generate(\n",
    "            input_ids = input_ids,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=200,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "            retrieval_embeds = relevant_embedding.unsqueeze(0),\n",
    "        )\n",
    "    answer = llm_tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity function\n",
    "def compute_text_perplexity(text: str, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Return the approximate perplexity of a text string using next-token probabilities.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "    input_ids = encodings.input_ids\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def split_by_conjunctions_spacy(text):\n",
    "    \"\"\"\n",
    "    Splits text into phrases at tokens identified as coordinating conjunctions or commas\n",
    "    using spaCy's dependency parser. Final phrases have punctuation removed and extra\n",
    "    whitespace stripped.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    phrases = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        current_tokens = []\n",
    "        for token in sent:\n",
    "            # If token is a coordinating conjunction (dep_ == \"cc\") or a comma, finish the phrase.\n",
    "            if token.dep_ == \"cc\" or token.text == \",\":\n",
    "                if current_tokens:\n",
    "                    phrase = \" \".join([t.text for t in current_tokens]).strip()\n",
    "                    # Remove punctuation from the phrase.\n",
    "                    phrase = phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "                    phrases.append(phrase)\n",
    "                    current_tokens = []\n",
    "            else:\n",
    "                current_tokens.append(token)\n",
    "        # Append any remaining tokens as the last phrase.\n",
    "        if current_tokens:\n",
    "            phrase = \" \".join([t.text for t in current_tokens]).strip()\n",
    "            phrase = phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "            phrases.append(phrase)\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "\n",
    "def llm_surprise_check_phrases(story: str, gist: str, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Splits the story into phrases (using our modified split function),\n",
    "    then computes the perplexity for each phrase with a given LLM.\n",
    "    Returns a list of tuples (phrase, perplexity) sorted descending by perplexity.\n",
    "    \"\"\"\n",
    "    phrases = split_by_conjunctions_spacy(story)\n",
    "    results = []\n",
    "    for phrase in phrases:\n",
    "        prompt = f\"{gist}\\n\\n{phrase}\"\n",
    "        ppl = compute_text_perplexity(prompt, model, tokenizer, device)\n",
    "        results.append((phrase, ppl))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in unexpected details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the documents to process\n",
    "documents = stories_subset\n",
    "datastore, doc_embeds, xrag_embeds = prepare_datastore(documents)\n",
    "\n",
    "# We have four detail levels to request\n",
    "n_details_list = [0, 1, 5]\n",
    "\n",
    "# Prepare a dictionary to store recalled stories for each detail level\n",
    "recalled_stories_dict = {n: [] for n in n_details_list}\n",
    "memory_sizes_dict = {n: [] for n in n_details_list}\n",
    "details_dict = {n: [] for n in n_details_list}\n",
    "\n",
    "for doc in documents:\n",
    "    print(\"\\nOriginal story:\", doc)\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # 1) Base question: no specific details\n",
    "    first_line = doc[0:50]#doc.split('.')[0]\n",
    "    question_0 = f\"{first_line}... What happened (in detail)?\"# (in as much detail as possible)?\"\n",
    "    print(\"\\nQuestion (0 details):\", question_0)\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    # Prepare & answer\n",
    "    prompt_0, query_embed_0 = prepare_prompt(question=question_0)\n",
    "    relevant_embedding_0 = get_top_match(query_embed_0, doc_embeds)\n",
    "    answer_0 = get_answer(prompt_0, relevant_embedding_0)\n",
    "    print(\"\\nAnswer (0 details):\", answer_0)\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    # Save the 0-detail answer\n",
    "    recalled_stories_dict[0].append(answer_0)\n",
    "    memory_sizes_dict[0].append(1)\n",
    "\n",
    "    # Determine surprising or important details used\n",
    "    details = llm_surprise_check_phrases(doc, answer_0, llm, llm_tokenizer)\n",
    "    # Strip whitespace from each detail\n",
    "    details = [d[0].strip() for d in details]\n",
    "\n",
    "    # For each of the other detail levels, build a question & get the answer\n",
    "    for n in [1, 5]:\n",
    "        # Slice the first n details\n",
    "        details_subset = details[:n]\n",
    "\n",
    "        details_str = \", \".join(details_subset)\n",
    "\n",
    "        # Build the question\n",
    "        question_n = (\n",
    "            f\"{first_line}... What happened (in detail)? \"# (in as much detail as possible)? \"\n",
    "            f\"Other information: {details_str}\"\n",
    "        )\n",
    "        print(f\"\\nQuestion ({n} details):\", question_n)\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "        # Prepare & answer\n",
    "        prompt_n, query_embed_n = prepare_prompt(question=question_n)\n",
    "        relevant_embedding_n = get_top_match(query_embed_n, doc_embeds)\n",
    "        answer_n = get_answer(prompt_n, relevant_embedding_n)\n",
    "        print(f\"\\nAnswer ({n} details):\", answer_n)\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "        # Save the answer for detail level n\n",
    "        recalled_stories_dict[n].append(answer_n)\n",
    "\n",
    "        # Tokenise each detail and sum their lengths\n",
    "        total_detail_tokens = sum(\n",
    "            len(llm_tokenizer(detail, return_tensors='pt')['input_ids'][0])\n",
    "            for detail in details_subset\n",
    "        )\n",
    "\n",
    "        memory_length = 1 + total_detail_tokens \n",
    "\n",
    "        memory_sizes_dict[n].append(memory_length)\n",
    "\n",
    "        details_dict[n].append(details_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recalled_stories.pkl\", \"wb\") as f:\n",
    "    pickle.dump(recalled_stories_dict, f)\n",
    "\n",
    "with open(\"recalled_stories_details.pkl\", \"wb\") as f:\n",
    "    pickle.dump(details_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get imagined stories as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side='left')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,  # or fp16 if you prefer\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval().to(device)\n",
    "\n",
    "def generate_imagined_story(original_story: str, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Takes the original story, extracts the first line (or first sentence)\n",
    "    and asks the base Mistral model to continue it.\n",
    "    \"\"\"\n",
    "    first_line = original_story[:50] #.split('.')[0].strip() + \".\"  \n",
    "    # Simple instruct prompt for Mistral:\n",
    "    #prompt = f\"[INST] {first_line}. Continue the story. [/INST] {first_line}. \"\n",
    "    prompt = f\"[INST] {first_line}... Continue the story. [/INST]\"\n",
    "\n",
    "    input_ids = base_tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=base_tokenizer.pad_token_id,\n",
    "        )\n",
    "    # Strip away the original prompt portion\n",
    "    decoded = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    # Combine first line + the newly generated text as the \"imagined\" story\n",
    "    # (Or you can just use the newly generated portion alone—your choice.)\n",
    "    return decoded.strip()\n",
    "\n",
    "def generate_full_detail_story(original_story: str, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Takes the original story, extracts the first line (or first sentence)\n",
    "    and asks the base Mistral model to continue it.\n",
    "    \"\"\"\n",
    "    first_line = original_story[:50] #.split('.')[0].strip() + \".\"  \n",
    "    # Simple instruct prompt for Mistral:\n",
    "    #prompt = f\"[INST] {first_line}. Continue the story. [/INST] {first_line}. \"\n",
    "    prompt = f\"[INST] Refer to the background document and answer the questions: Background: {original_story} \\n Question: {first_line}... Continue the story. [/INST]\"\n",
    "\n",
    "    input_ids = base_tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=base_tokenizer.pad_token_id,\n",
    "        )\n",
    "    # Strip away the original prompt portion\n",
    "    decoded = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    # Combine first line + the newly generated text as the \"imagined\" story\n",
    "    # (Or you can just use the newly generated portion alone—your choice.)\n",
    "    return decoded.strip()\n",
    "\n",
    "emb_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def get_embedding(text):\n",
    "    # SentenceTransformer returns a NumPy array\n",
    "    np_vec = emb_model.encode([text])[0]\n",
    "    # Convert to a Torch tensor on the same device\n",
    "    # (If you’re using CPU only, remove \".to(device)\" or set device='cpu')\n",
    "    return torch.tensor(np_vec, dtype=torch.float, device=device)\n",
    "\n",
    "def cosine_similarity(emb_a, emb_b):\n",
    "    emb_a_norm = emb_a / emb_a.norm(dim=-1, keepdim=True)\n",
    "    emb_b_norm = emb_b / emb_b.norm(dim=-1, keepdim=True)\n",
    "    return torch.sum(emb_a_norm * emb_b_norm, dim=-1)\n",
    "\n",
    "\n",
    "def cosine_distance(emb_a, emb_b):\n",
    "    return 1 - cosine_similarity(emb_a, emb_b)\n",
    "\n",
    "\n",
    "stories = stories_subset\n",
    "\n",
    "# Generate \"imagined\" stories using the base model\n",
    "imagined_stories = []\n",
    "for story in stories:\n",
    "    imagined_story = generate_imagined_story(story)\n",
    "    imagined_stories.append(imagined_story)\n",
    "    print(imagined_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distances from recalled story and memory sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels and levels\n",
    "labels = [\"Imagined\", \"Gist only\", \"1 detail\", \"5 details\", \"Full detail\"]\n",
    "n_details_list = [0, 1, 5]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.45\n",
    "\n",
    "# Initialize similarity storage\n",
    "distances = {n: [] for n in n_details_list}\n",
    "imagined_sims = []\n",
    "full_detail_sims = []\n",
    "\n",
    "# Token lengths for full detail stories\n",
    "full_detail_token_lens = []\n",
    "\n",
    "# Loop over all stories\n",
    "for i, original_text in enumerate(stories):\n",
    "    imagined_text = imagined_stories[i]\n",
    "    full_detail_text = original_text\n",
    "    \n",
    "    # Retrieve recalled stories\n",
    "    recalled_texts = {n: recalled_stories_dict[n][i] for n in n_details_list}\n",
    "    \n",
    "    # Determine the shortest shared truncation length\n",
    "    all_texts = [original_text, imagined_text, full_detail_text] + list(recalled_texts.values())\n",
    "    min_len = min(len(t) for t in all_texts)\n",
    "    \n",
    "    emb_original = get_embedding(original_text[:min_len])\n",
    "    emb_imagined = get_embedding(imagined_text[:min_len])\n",
    "    emb_full_detail = get_embedding(full_detail_text[:min_len])\n",
    "    \n",
    "    # Imagined and Full Detail similarities\n",
    "    imagined_sim = 1 - cosine_distance(emb_original, emb_imagined).item()\n",
    "    full_detail_sim = 1\n",
    "    imagined_sims.append(imagined_sim)\n",
    "    full_detail_sims.append(full_detail_sim)\n",
    "    \n",
    "    # Token length of full detail memory\n",
    "    full_detail_len = len(base_tokenizer(original_text, return_tensors='pt')['input_ids'][0])\n",
    "    full_detail_token_lens.append(full_detail_len)\n",
    "    \n",
    "    # Recalled memory similarities\n",
    "    for n in n_details_list:\n",
    "        recalled_text = recalled_texts[n]\n",
    "        emb_recalled = get_embedding(recalled_text[:min_len])\n",
    "        sim = 1 - cosine_distance(emb_original, emb_recalled).item()\n",
    "        distances[n].append(sim)\n",
    "\n",
    "# === Aggregate statistics ===\n",
    "\n",
    "# Similarities\n",
    "imagined_similarity = np.mean(imagined_sims)\n",
    "imagined_similarity_sem = sem(imagined_sims)\n",
    "\n",
    "full_detail_similarity = np.mean(full_detail_sims)\n",
    "full_detail_similarity_sem = sem(full_detail_sims)\n",
    "\n",
    "mean_similarities = [imagined_similarity] + [np.mean(distances[n]) for n in n_details_list] + [full_detail_similarity]\n",
    "sem_similarities = [imagined_similarity_sem] + [sem(distances[n]) for n in n_details_list] + [full_detail_similarity_sem]\n",
    "\n",
    "# Memory sizes\n",
    "mean_memory_sizes = [0] + [np.mean(memory_sizes_dict[n]) for n in n_details_list] + [np.mean(full_detail_token_lens)]\n",
    "sem_memory_sizes = [0] + [sem(memory_sizes_dict[n]) for n in n_details_list] + [sem(full_detail_token_lens)]\n",
    "\n",
    "# === Plotting ===\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(5, 2.3))\n",
    "\n",
    "# Plot similarity (left y-axis)\n",
    "bars1 = ax1.bar(x - width/2, mean_similarities, width, yerr=sem_similarities, capsize=5, \n",
    "                label='Similarity to Original', alpha=0.5)\n",
    "ax1.set_ylabel('Similarity to original')\n",
    "ax1.set_ylim(0.6, 1.03)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "# Create second axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot memory sizes (right y-axis)\n",
    "bars2 = ax2.bar(x + width/2, mean_memory_sizes, width, yerr=sem_memory_sizes, capsize=5,\n",
    "                color='red', alpha=0.5, label='Memory Size (tokens)')\n",
    "ax2.set_ylabel('Memory size (tokens)')\n",
    "ax2.set_ylim(0, max(mean_memory_sizes) + 4)\n",
    "\n",
    "# Match axis colors\n",
    "ax1.tick_params(axis='y', colors='C0')\n",
    "ax1.yaxis.label.set_color('C0')\n",
    "\n",
    "ax2.tick_params(axis='y', colors='red')\n",
    "ax2.yaxis.label.set_color('red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grouped_similarity_memory_with_imagined_and_full_detail.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recalled_stories.pkl\", \"rb\") as f:\n",
    "    recalled_stories_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(\"REAL STORY:\")\n",
    "    print(stories_subset[i])\n",
    "    print(\"------------------------------\")\n",
    "    print(\"DETAILS:\")\n",
    "    print(details_dict[5][i])\n",
    "    print(\"------------------------------\")    \n",
    "    print(\"GIST:\")\n",
    "    print(recalled_stories_dict[0][i])\n",
    "    print(\"------------------------------\")\n",
    "    print(\"ONE DETAIL:\")\n",
    "    print(recalled_stories_dict[1][i])\n",
    "    print(\"------------------------------\")\n",
    "    print(\"FIVE DETAILS:\")\n",
    "    print(recalled_stories_dict[5][i])\n",
    "    print(\"------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
