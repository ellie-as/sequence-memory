{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating consolidation of narratives in the full model\n",
    "\n",
    "Note that the contents of the xRAG directory are copied from https://github.com/Hannibal046/xRAG (so our results are reproducible in case of future changes to this repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import sys\n",
    "import spacy\n",
    "import string\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    set_seed,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "sys.path.append('xRAG')\n",
    "from src.model import SFR,XMistralForCausalLM\n",
    "from src.language_modeling.utils import get_retrieval_embeds, XRAG_TOKEN\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories():\n",
    "    df = pd.read_csv('stories_train.csv')\n",
    "    df['combined'] = df[[f'sentence{i}' for i in range(1,6)]].astype(str).agg(' '.join, axis=1)\n",
    "    return df['combined'].tolist()\n",
    "\n",
    "stories = get_stories()\n",
    "random.Random(123).shuffle(stories)\n",
    "stories_subset = stories[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation in the extended model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recalled_stories.pkl\", \"rb\") as f:\n",
    "    recalled_stories_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mistral_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "def preprocess_data(texts, tokenizer):\n",
    "    \"\"\"Tokenize the list of strings for a causal LM.\"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    return inputs\n",
    "\n",
    "class PrintContinuationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback that prints the model's continuation\n",
    "    of the first line of each test story every N steps.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainer,\n",
    "        test_stories,\n",
    "        tokenizer,\n",
    "        print_frequency=50,\n",
    "        max_new_tokens=100,\n",
    "        max_stories_to_print=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.test_stories = test_stories\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_frequency = print_frequency\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.max_stories_to_print = max_stories_to_print\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        \"\"\"Called at the end of each training step.\"\"\"\n",
    "        global_step = state.global_step\n",
    "\n",
    "        if global_step > 0 and (global_step % self.print_frequency == 0):\n",
    "            model = self.trainer.model\n",
    "            device = next(model.parameters()).device\n",
    "            model.eval()\n",
    "\n",
    "            print(f\"\\n[PrintContinuationCallback] Step={global_step} - Generating test story completions:\")\n",
    "            for i, story in enumerate(self.test_stories):\n",
    "                if i >= self.max_stories_to_print:\n",
    "                    break  # avoid printing too many stories\n",
    "\n",
    "                # Take the first line as a prompt\n",
    "                first_line = story.split('.')[0].strip()\n",
    "                inputs = self.tokenizer(first_line, return_tensors='pt').to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=self.max_new_tokens,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                continuation = generated_text[len(first_line):].strip()\n",
    "\n",
    "                print(f\"  Story {i+1} prompt: {first_line}\")\n",
    "                print(f\"  => Continuation: {continuation}\\n\")\n",
    "                print(f\"  Real story (for reference): {story}\\n\")\n",
    "\n",
    "def finetune_model(\n",
    "    training_data,\n",
    "    test_data,\n",
    "    model_name='finetuned_model',\n",
    "    seed=123\n",
    "):\n",
    "    # Clean output directory\n",
    "    !rm -rf {model_name}\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Build train & test datasets\n",
    "    train_dataset = Dataset.from_dict({\"text\": training_data})\n",
    "    train_dataset = train_dataset.map(lambda x: preprocess_data(x[\"text\"], tokenizer), batched=True)\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_data})\n",
    "    test_dataset = test_dataset.map(lambda x: preprocess_data(x[\"text\"], tokenizer), batched=True)\n",
    "\n",
    "    # Load 4-bit model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        mistral_model,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"k_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=30,\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy='epoch',\n",
    "        seed=seed,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "    data_collator = DefaultDataCollator()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Evaluate on both train/test *before* training\n",
    "    print(\"\\n=== Evaluating before training ===\")\n",
    "    pre_train_loss_metrics = trainer.evaluate(eval_dataset=train_dataset)\n",
    "    pre_train_loss = pre_train_loss_metrics[\"eval_loss\"]\n",
    "    print(f\"Initial Train Loss: {pre_train_loss:.4f}\")\n",
    "\n",
    "    pre_test_loss_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    pre_test_loss = pre_test_loss_metrics[\"eval_loss\"]\n",
    "    print(f\"Initial Test Loss:  {pre_test_loss:.4f}\")\n",
    "\n",
    "    continuation_callback = PrintContinuationCallback(\n",
    "        trainer=trainer,\n",
    "        test_stories=test_data, \n",
    "        tokenizer=tokenizer,\n",
    "        print_frequency=20,\n",
    "        max_new_tokens=100,\n",
    "        max_stories_to_print=5\n",
    "    )\n",
    "    trainer.add_callback(continuation_callback)\n",
    "\n",
    "    # Train:\n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model and tokeniser:\n",
    "    model.save_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    return trainer, pre_train_loss, pre_test_loss\n",
    "\n",
    "trainer, init_train_loss, init_test_loss = finetune_model(\n",
    "    training_data=recalled_stories_dict[0],\n",
    "    test_data=stories_subset,\n",
    "    model_name=\"finetuned_model\",\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "\n",
    "train_steps, train_losses = [], []\n",
    "eval_steps, eval_losses = [], []\n",
    "\n",
    "train_steps.append(-1)\n",
    "train_losses.append(init_train_loss)\n",
    "eval_steps.append(-1)\n",
    "eval_losses.append(init_test_loss)\n",
    "\n",
    "for entry in logs:\n",
    "    if \"loss\" in entry and \"eval_loss\" not in entry:\n",
    "        step = entry[\"step\"]\n",
    "        loss_val = entry[\"loss\"]\n",
    "        train_steps.append(step)\n",
    "        train_losses.append(loss_val)\n",
    "    elif \"eval_loss\" in entry:\n",
    "        # This is an eval loss log\n",
    "        step = entry[\"step\"]\n",
    "        loss_val = entry[\"eval_loss\"]\n",
    "        eval_steps.append(step)\n",
    "        eval_losses.append(loss_val)\n",
    "\n",
    "# 10) Plot the final figure\n",
    "plt.figure(figsize=(2.7,2.3))\n",
    "plt.plot(train_steps, train_losses, label=\"Encoded story\", color='r')\n",
    "plt.plot(eval_steps, eval_losses, label=\"Original story\", color='b')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Prediction error\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Optionally save the plot\n",
    "plt.savefig('loss_over_time.png', dpi=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
