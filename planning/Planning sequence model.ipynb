{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd5d68a-4289-41f7-838e-495ecf0d995d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Simulating Vikbladh et al. (2024) consolidation task\n",
    "\n",
    "Consolidation can be thought of as training a neocortical generative / predictive model on replayed hippocampal memories. For sequential data, the generative model could correspond to an autoregressive sequence model like GPT-2 that learns to predict the next item in the sequence (by minimising the prediction error on sequences from the training data). The stimuli for Oliver's task can be represented as sequences of form 'START: yellow fruit, STOP: red, REWARD: animal, SEQUENCE: yellow fruit (-1), green animal (2), red animal (2)', which makes it straightforward to train GPT-2.\n",
    "\n",
    "This notebook simulates the task as follows:\n",
    "* Pre-train model so that it learns the *rules* of task\n",
    "* Train on stimuli for task, representing consolidation\n",
    "* Compare generative model accept / reject performance before and after consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1a12d-ed03-4a17-960d-0a6863681170",
   "metadata": {},
   "source": [
    "#### Installation / imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05917c15-2dc4-4d88-9ab0-5135187faaa3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers --upgrade\n",
    "! pip install accelerate evaluate wonderwords --upgrade\n",
    "! pip install huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7afc1-f112-4b77-800e-b2f47f67e159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "from wonderwords import RandomWord\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147a7d7-2f31-4b66-ab1d-e22ac6ad9c38",
   "metadata": {},
   "source": [
    "Define a class for loading a model from a directory, and generating outputs given some input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9679c0-8ef3-415e-91d8-6eb1aa55ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "\n",
    "    def continue_input(self, input_sequence, max_length=200, num_return_sequences=1, no_repeat_ngram_size=0,\n",
    "                       do_sample=False, temperature=0.7, num_beams=1):\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "        max_length = len(input_ids[0]) + 100        \n",
    "\n",
    "        # Generate text\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        sequence = output[0].tolist()\n",
    "        text = self.tokenizer.decode(sequence)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464ff8e-5309-483a-bd83-0a28dfbba60d",
   "metadata": {},
   "source": [
    "#### Pre-train model on arbitrary stimuli to learn rules of task\n",
    "\n",
    "The get_random_stimuli() function generates a random set of nouns and adjectives (3 for each by default). The stimuli are all possible combinations, e.g. for the adjectives ABC and nouns DEF, the stimuli are AD, AE, AF, BD, etc. The get_stimuli() function is the equivalent but for Oliver's task stimuli. \n",
    "\n",
    "The get_reward() function predicts reward points for a sequence of stimuli. Given a list of stimuli in random order, the stimulus at which the sequence starts, the adjective at which the sequence ends, and the noun that gives 2 points of reward, the function returns a list of stimuli and their rewards, e.g. ['small chair (2)', 'angry chair (2)', 'metal spoon (-1)']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90dc38-13d5-4915-b7ab-66550bf55c80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_stimuli(n=3):\n",
    "    r = RandomWord()\n",
    "    adjectives = [r.word(include_parts_of_speech=[\"adjectives\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "    nouns = [r.word(include_parts_of_speech=[\"nouns\"]).replace(\" \", \"_\") for _ in range(n)]\n",
    "\n",
    "    stimuli = []\n",
    "    for i, noun in enumerate(nouns):\n",
    "        for adjective in adjectives:\n",
    "            stimuli.append(f\"{adjective} {noun}\")\n",
    "\n",
    "    return stimuli, nouns, adjectives\n",
    "\n",
    "def get_stimuli():\n",
    "    stimuli = [\"red animal\", \n",
    "               \"green animal\", \n",
    "               \"yellow animal\", \n",
    "               \"red vehicle\", \n",
    "               \"green vehicle\", \n",
    "               \"yellow vehicle\", \n",
    "               \"red fruit\", \n",
    "               \"green fruit\", \n",
    "               \"yellow fruit\"]\n",
    "    \n",
    "    objects = [word.split()[1] for word in stimuli]\n",
    "    colours = list(set([word.split()[0] for word in stimuli]))\n",
    "    return stimuli, objects, colours\n",
    "\n",
    "def shuffle_stimuli(stimuli):\n",
    "    random.shuffle(stimuli)\n",
    "    return stimuli\n",
    "\n",
    "def get_reward(stimuli, start, stop, reward):\n",
    "    \"\"\"Predict reward points for a sequence of stimuli.\n",
    "    \n",
    "    Args:\n",
    "        stimuli (list): List of stimuli in random order.\n",
    "        start (str): Object at which the sequence starts.\n",
    "        stop (str): Colour at which the sequence ends.\n",
    "        reward (str): Category of object that brings 2 points of reward.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Reward points descriptions for the sequence.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    sequence_started = False\n",
    "    \n",
    "    index = 0  # start index\n",
    "    while True:\n",
    "        stim = stimuli[index % len(stimuli)]\n",
    "        colour, obj = stim.split()\n",
    "\n",
    "        if not sequence_started:\n",
    "            if stim == start:\n",
    "                sequence_started = True\n",
    "            index += 1\n",
    "            continue\n",
    "        else:\n",
    "            if obj == reward:\n",
    "                points.append(f\"{stim} (2)\")\n",
    "            else:\n",
    "                points.append(f\"{stim} (-1)\")\n",
    "    \n",
    "            if colour == stop:\n",
    "                break\n",
    "\n",
    "        index += 1  # move to the next stimulus\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83366812-1e46-4304-a690-d3cadf2cf35e",
   "metadata": {},
   "source": [
    "We now generate training and test data for the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b93d3f-dd38-40bf-a3e5-cfce37e89d6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_strs = []\n",
    "for i in range(1000):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    strs_for_stimuli = []\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                strs_for_stimuli.append(training_str)\n",
    "    shuffle(strs_for_stimuli)\n",
    "    training_strs.extend(strs_for_stimuli)\n",
    "\n",
    "testing_strs = []\n",
    "for i in range(10):\n",
    "    stimuli, objects, colours = get_random_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                testing_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                testing_strs.append(testing_str)\n",
    "\n",
    "print(f\"{len(training_strs)} sequences of arbitrary stimuli generated for pre-training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a49df-cb78-4b49-aa1e-6f894f48ca63",
   "metadata": {},
   "source": [
    "The function below runs a script to fine-tune a gpt-2 model on the arbitrary stimuli.\n",
    "\n",
    "The name_or_path argument is which model to fine-tune from. In the pre-training stage, this will be set to 'gpt2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2433c5-aa7c-496c-a5b7-849674956a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=3,\n",
    "                       output_dir='./clm_script',\n",
    "                       save_steps=100,\n",
    "                       lr=5e-05, \n",
    "                       seed=0):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    ! python3 ./run_clm.py \\\n",
    "        --model_name_or_path {name_or_path} \\\n",
    "        --train_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --validation_file {os.path.join(output_dir, 'train.txt')} \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --per_device_eval_batch_size 1 \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --overwrite_output_dir \\\n",
    "        --num_train_epochs {num_epochs} \\\n",
    "        --save_strategy 'steps' \\\n",
    "        --save_steps {save_steps} \\\n",
    "        --learning_rate {lr} \\\n",
    "        --seed {seed}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e5c52-5e6a-4769-983b-23a1117d1c08",
   "metadata": {},
   "source": [
    "Shuffle the data, write it to train.txt and test.txt files, and train gpt2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039427a-4609-4b49-b39c-ce4cbedccaf9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf rule_model\n",
    "!mkdir rule_model\n",
    "\n",
    "text_file = open(\"rule_model/train.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(training_strs))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"rule_model/test.txt\", \"w\")\n",
    "n = text_file.write('\\n'.join(testing_strs))\n",
    "text_file.close()\n",
    "\n",
    "train_model_script(name_or_path='gpt2', output_dir='rule_model', num_epochs=10, save_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a9fe0-84c8-4d80-b51f-1cc503980685",
   "metadata": {},
   "source": [
    "Test the output with some random start / stop / reward conditions not seen in the training data to see if it's learned the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d33e27-6eb0-45a7-98e1-bb9d6eee3f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT(base_model='rule_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dbdfa-a65b-4033-bbd0-455d74eff664",
   "metadata": {},
   "source": [
    "Can the model generalise the rules to new stimuli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99c4b7-f598-45b5-b891-6c808a1836fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model.continue_input(\"START: blue table, STOP: green, REWARD: bug, SEQUENCE:\", do_sample=False)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb1926-5a4a-49d8-9a91-cd8a80410df0",
   "metadata": {},
   "source": [
    "#### Simulate the task\n",
    "\n",
    "The simulate_task() function does the following:\n",
    "* First it clears down the model directory.\n",
    "* Then it shuffles the stimuli for the trial into a random order.\n",
    "* Then it generates all possible start / stop / reward combinations and gets the sequence for each from the get_reward() function.\n",
    "* As above, it turns these into strings with format f\"START: ..., STOP: ..., REWARD: ..., SEQUENCE: ...\"\n",
    "* In the reward revaluation case, we train on 2/3 of the reward categories in each trial, and test on a held-out reward category. In the transition revaluation case, we train on 2/3 of the stop colours in each trial, and test on a held-out transition category. \n",
    "* The training and test data is saved to files, and a model is trained by calling the train_model_script() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75e56c-47d1-408b-8eae-244d03808b80",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_task(seed=0, num_new=1000, num_epochs=3):\n",
    "    random.seed(seed)\n",
    "    training_strs = []\n",
    "    \n",
    "    # get stimuli etc and shuffle them\n",
    "    stimuli, objects, colours = get_stimuli()\n",
    "    stimuli = shuffle_stimuli(stimuli)\n",
    "    print(f\"Stimuli: {stimuli}\")\n",
    "    \n",
    "    for stim in stimuli:\n",
    "        for colour in colours:\n",
    "            for obj in objects:\n",
    "                start = stim\n",
    "                stop = colour\n",
    "                reward = obj\n",
    "                instruction_str = f\"START: {start}, STOP: {stop}, REWARD: {reward},\"\n",
    "                points = get_reward(stimuli, start, stop, reward)\n",
    "                training_str = instruction_str + \" SEQUENCE: \" + \", \".join(points)\n",
    "                training_strs.append(training_str)\n",
    "\n",
    "    training_strs = list(set(training_strs))\n",
    "\n",
    "    random_category = random.choice(objects)\n",
    "    random_colour = random.choice(colours)\n",
    "    train_set = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' in s]\n",
    "    test_set_reward = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' in s]\n",
    "    test_set_transition = [s for s in training_strs if f'REWARD: {random_category}' in s and f'STOP: {random_colour}' not in s]\n",
    "    test_set_both = [s for s in training_strs if f'REWARD: {random_category}' not in s and f'STOP: {random_colour}' not in s]\n",
    "    \n",
    "    # oversampling trick to avoid overfitting to sequence order\n",
    "    train_set = np.random.choice(train_set, num_new).tolist()\n",
    "\n",
    "    output_dir = f'clm_script_{seed}'\n",
    "    ! rm -rf {output_dir}\n",
    "    ! mkdir {output_dir}\n",
    "    \n",
    "    trial_info = {'stimuli': stimuli, 'train_reward': random_category, 'train_stop': random_colour}\n",
    "    with open(os.path.join(output_dir, 'trial_info.pkl'), 'wb') as handle:\n",
    "        pickle.dump(trial_info, handle)\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'train.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(train_set))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_reward.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_reward))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_transition.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_transition))\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(os.path.join(output_dir, 'test_both.txt'), \"w\")\n",
    "    n = text_file.write('\\n'.join(test_set_both))\n",
    "    text_file.close()\n",
    "\n",
    "    train_model_script(name_or_path='rule_model', \n",
    "                       num_epochs=num_epochs, \n",
    "                       output_dir=output_dir,\n",
    "                       save_steps=100,\n",
    "                       seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4b845-6a51-47b0-a593-ec90a2b62fcc",
   "metadata": {},
   "source": [
    "Reward and transition revaluation test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39c98e-0d49-4f6d-be58-cce58da8a4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accept_reject_choice(seq):\n",
    "    seq = seq[seq.index('SEQUENCE:') + len('SEQUENCE:'):]\n",
    "    if 'START' in seq:\n",
    "        seq = seq[0:seq.index('START')]\n",
    "    print(seq)\n",
    "    try:\n",
    "        numbers = re.findall(r'\\([A-Za-z0-9_-]+\\)', seq)\n",
    "        numbers = [int(num.replace('(', '').replace(')', '')) for num in numbers]\n",
    "        print(numbers)\n",
    "    except Exception as e:\n",
    "        \"Couldn't convert to int, setting list to []\"\n",
    "        numbers = []\n",
    "    if sum(numbers) > 0:\n",
    "        return 1\n",
    "    elif sum(numbers) < 0:\n",
    "        return 0\n",
    "    elif sum(numbers) == 0:\n",
    "        return 0.5\n",
    "\n",
    "def test_revaluation(seqs, model):\n",
    "    result_bools = []\n",
    "    result_preds = []\n",
    "    for seq in seqs:\n",
    "        print(\"Get true accept / reject:\")\n",
    "        true_a_v_r = get_accept_reject_choice(seq)\n",
    "        print(true_a_v_r)\n",
    "        input_str = seq[0:seq.index('SEQUENCE:') + len('SEQUENCE:')]\n",
    "        continuation = model.continue_input(input_str, do_sample=False)\n",
    "        print(\"Get pred accept / reject:\")\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation)\n",
    "        print(pred_a_v_r)\n",
    "        result_preds.append(pred_a_v_r)\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            print(\"match\")\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            print(\"no match\")\n",
    "            result_bools.append(0)\n",
    "    return (result_bools, result_preds)\n",
    "\n",
    "def get_len(seq):\n",
    "    return seq.count(',') - 2\n",
    "\n",
    "def get_mean(val):\n",
    "    return np.mean(val[0])\n",
    "\n",
    "def num_sort(test_string):\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]\n",
    "\n",
    "def test_consolidation(base_dir, task_type='transition'):\n",
    "    print(task_type)\n",
    "    print(\"------------------------\")\n",
    "    checkpoints = glob.glob(os.path.join(base_dir, 'checkpoint*'))\n",
    "    model_dirs = ['rule_model'] + checkpoints \n",
    "    print(model_dirs)\n",
    "\n",
    "    with open(os.path.join(base_dir, f'test_{task_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        seqs = sorted(list(set(seqs)))\n",
    "\n",
    "    results = {}\n",
    "    for model_dir in model_dirs:\n",
    "        print(model_dir)\n",
    "        model = GPT(base_model=model_dir)\n",
    "        results[os.path.basename(model_dir)] = test_revaluation(seqs, model)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87448-18f2-4fb4-b2b9-a46ba8603c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(file_name):\n",
    "    with open('data/' + file_name, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.dropna(axis='columns')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(get_mean)\n",
    "\n",
    "    stats_to_plot = df.describe()\n",
    "    \n",
    "    return stats_to_plot\n",
    "\n",
    "def get_step_num(string):\n",
    "    # Regular expression to match 'checkpoint-' followed by one or more digits\n",
    "    match = re.match(r'checkpoint-(\\d+)', string)\n",
    "    \n",
    "    # If there's a match and it's at the start of the string\n",
    "    if match and match.start() == 0:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def produce_plot(description_string='-'):\n",
    "    files = ['results_reward.pkl', 'results_transition.pkl', 'results_both.pkl']\n",
    "    labels = ['Reward revaluation', 'Transition revaluation', 'Both revaluation']\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    \n",
    "    for ind, file in enumerate(files):\n",
    "        stats_to_plot = get_data_from_file(file) \n",
    "        cols = stats_to_plot.columns.tolist()\n",
    "        \n",
    "        xs = [get_step_num(c) for c in cols]\n",
    "        ys = [stats_to_plot[c]['mean'] for c in cols]\n",
    "        yerrs = [stats_to_plot[c]['std'] / math.sqrt(stats_to_plot[c]['count']) for c in cols] \n",
    "        sorted_pairs = sorted(zip(xs, ys, yerrs), key=lambda x: x[0])  # Sort by xs\n",
    "        sorted_xs, sorted_ys, sorted_yerrs = zip(*sorted_pairs)\n",
    "        \n",
    "        # Use errorbar instead of plot to include error margins\n",
    "        plt.errorbar(sorted_xs[:-1], \n",
    "                     sorted_ys[:-1], \n",
    "                     yerr=sorted_yerrs[:-1],  # Specify the error bars\n",
    "                     marker='o', \n",
    "                     capsize=5,\n",
    "                     label=labels[ind])\n",
    "    \n",
    "    plt.yticks(np.linspace(0, 1, num=11))\n",
    "    plt.ylabel('Accept / reject accuracy')\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'revaluation over time{description_string}.png', dpi=500, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1dd32-6cf2-4ffb-9336-47aecea4a9ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_new = 1000\n",
    "num_epochs = 20\n",
    "\n",
    "for i in range(5):\n",
    "    simulate_task(seed=i, \n",
    "                  num_new=num_new, \n",
    "                  num_epochs=num_epochs)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632c57f-61de-4ea4-a4ad-04f7482278f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results_transition = []\n",
    "all_results_reward = []\n",
    "all_results_both = []\n",
    "\n",
    "for i in range(5):\n",
    "    base_dir = f'clm_script_{i}'\n",
    "    \n",
    "    results = test_consolidation(base_dir, task_type='transition')\n",
    "    all_results_transition.append(results)\n",
    "\n",
    "    results = test_consolidation(base_dir, task_type='reward')\n",
    "    all_results_reward.append(results)\n",
    "\n",
    "    results = test_consolidation(base_dir, task_type='both')\n",
    "    all_results_both.append(results)\n",
    "\n",
    "with open('data/results_transition.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_transition, f)\n",
    "with open('data/results_reward.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_reward, f)\n",
    "with open('data/results_both.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results_both, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ca849-2296-4cf2-933a-d9e6e46f67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plot(description_string='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20a3e4-436c-478f-8145-4492a491fbab",
   "metadata": {},
   "source": [
    "#### Regression coefficients approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba8f86-71f1-4c9b-a631-ba52e38042ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pred_reward_for_strategy(seq, output_dir, strategy):\n",
    "    with open(os.path.join(output_dir, 'trial_info.pkl'), 'rb') as handle:\n",
    "        trial_info = pickle.load(handle)\n",
    "        stimuli = trial_info['stimuli']\n",
    "        train_stop = trial_info['train_stop']\n",
    "        train_reward = trial_info['train_reward']\n",
    "    \n",
    "    test_start = seq[0:seq.index(', STOP')].replace('START: ', '')\n",
    "    test_stop = seq[seq.index('STOP'):seq.index('REWARD')].replace('STOP: ', '').replace(', ', '')\n",
    "    test_reward = seq[seq.index('REWARD'):seq.index('SEQUENCE')].replace('REWARD: ', '').replace(', ', '')\n",
    "    \n",
    "    print(f'Train stop: {train_stop}, train reward: {train_reward}, test stop: {test_stop}, test reward: {test_reward}')\n",
    "    print(stimuli)\n",
    "    \n",
    "    if strategy == 'revaluate_reward':\n",
    "        # get train stop, test reward\n",
    "        outcome = get_accept_reject(stimuli, test_start, train_stop, test_reward)\n",
    "    if strategy == 'revaluate_transition':\n",
    "        # get train reward, test stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, test_stop, train_reward)\n",
    "    if strategy == 'revaluate_both':\n",
    "        # get test reward, test stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, test_stop, test_reward)\n",
    "    if strategy == 'no_revaluation':\n",
    "        # get train reward, train stop\n",
    "        outcome = get_accept_reject(stimuli, test_start, train_stop, train_reward)\n",
    "    return outcome\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934985dc-a543-4179-888f-60bf56b94afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accept_reject(stimuli, start, stop, reward):\n",
    "\n",
    "    points = []\n",
    "    sequence_started = False\n",
    "    \n",
    "    index = 0  # start index\n",
    "    while True:\n",
    "        stim = stimuli[index % len(stimuli)]\n",
    "        colour, obj = stim.split()\n",
    "\n",
    "        if not sequence_started:\n",
    "            if stim == start:\n",
    "                sequence_started = True\n",
    "            index += 1\n",
    "            continue\n",
    "        else:\n",
    "            if obj == reward:\n",
    "                points.append(2)\n",
    "            else:\n",
    "                points.append(-1)\n",
    "    \n",
    "            if colour == stop:\n",
    "                break\n",
    "\n",
    "        index += 1  # move to the next stimulus\n",
    "\n",
    "    print(f\"Inferred points sequence for {start} / {stop} / {reward}: {points}\")\n",
    "    if sum(points) > 0:\n",
    "        return 1\n",
    "    if sum(points) < 0:\n",
    "        return 0\n",
    "    if sum(points) ==0:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02605ffc-db3d-4d50-b19a-73b20a5622de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_strategy(i, task_type='reward', strategy='no_revaluation'):\n",
    "    model_dir = f'./clm_script_{i}'\n",
    "    \n",
    "    with open(os.path.join(model_dir, f'test_{task_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        print(f\"{len(seqs)} seqs.\")\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        seqs = sorted(list(set(seqs)))\n",
    "        print(task_type, strategy)\n",
    "    \n",
    "    preds = []\n",
    "    for seq in seqs:\n",
    "        pred = get_pred_reward_for_strategy(seq, model_dir, strategy)\n",
    "        preds.append(pred)\n",
    "        print(seq)\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(\"......................\")\n",
    "    return np.asarray(preds)\n",
    "\n",
    "\n",
    "def get_strategy_df(task_type='transition'):\n",
    "    file_name = f'data/results_{task_type}.pkl'\n",
    "    with open(file_name, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.dropna(axis='columns')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: np.array(x[1]))\n",
    "\n",
    "    df[\"model_num\"] = df.index\n",
    "    print(df.index.tolist())\n",
    "\n",
    "    df['NR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='no_revaluation'))\n",
    "\n",
    "    df['RR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_reward'))\n",
    "\n",
    "    df['TR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_transition'))\n",
    "\n",
    "    df['BR_preds'] = df[\"model_num\"].apply(lambda x: get_strategy(x, \n",
    "                                                                  task_type=task_type, \n",
    "                                                                  strategy='revaluate_both'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115057e-fa6c-424b-9d9e-74e596080682",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = get_strategy_df(task_type='transition')\n",
    "df1 = get_strategy_df(task_type='reward')\n",
    "df3 = get_strategy_df(task_type='both')\n",
    "df = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29944d63-9a35-458a-bb18-35c25fb8dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "for model in ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300', 'checkpoint-400']:\n",
    "\n",
    "    targets = flatten_col(model)\n",
    "    feats = [flatten_col('RR_preds'),\n",
    "             flatten_col('TR_preds'),\n",
    "             flatten_col('NR_preds'),\n",
    "             flatten_col('BR_preds')]\n",
    "    feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(feats, targets)\n",
    "    print(model)\n",
    "    print(reg.coef_)\n",
    "    print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7ef9d-dd86-4e58-ad08-f5598dbcc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Function to plot the coefficients\n",
    "def plot_coefficients(models, feature_names):\n",
    "    coefs = []\n",
    "    intercepts = []\n",
    "    \n",
    "    for model in models:\n",
    "        targets = flatten_col(model)\n",
    "        feats = [flatten_col('RR_preds'),\n",
    "                 flatten_col('TR_preds'),\n",
    "                 flatten_col('NR_preds'),\n",
    "                 flatten_col('BR_preds')]\n",
    "        feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "        reg = LinearRegression(fit_intercept=True)\n",
    "        reg.fit(feats, targets)\n",
    "        \n",
    "        coefs.append(reg.coef_)\n",
    "        intercepts.append(reg.intercept_)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(models), figsize=(17, 3), sharey=True)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.bar(feature_names, coefs[idx])\n",
    "        ax.axhline(y=intercepts[idx], color='r', linestyle='--')\n",
    "        ax.set_title(models[idx])\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Coefficient Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['RR', 'TR', 'MF', 'MB']\n",
    "# Models\n",
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300', 'checkpoint-400', 'checkpoint-500']\n",
    "\n",
    "# Plot the coefficients\n",
    "plot_coefficients(models, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6497a6-cffb-4ce8-bad6-86244e5b16b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300', 'checkpoint-400']\n",
    "strategies = ['NR_preds', 'RR_preds', 'TR_preds', 'BR_preds']\n",
    "\n",
    "for model in models:\n",
    "    for strategy in strategies:\n",
    "        print(model, strategy)\n",
    "        print(pearsonr(flatten_col(model), flatten_col(strategy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c4846-6f97-4642-985a-8e8ef5923c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Define the new labels for models and strategies\n",
    "model_labels = ['0', '100', '200', '300', '400']\n",
    "strategy_labels = ['Model\\nfree', 'Reward\\nreval.', 'Transition\\nreval.', 'Model\\nbased']\n",
    "\n",
    "# Original model and strategy identifiers\n",
    "models = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300', 'checkpoint-400']\n",
    "strategies = ['NR_preds', 'RR_preds', 'TR_preds', 'BR_preds']\n",
    "\n",
    "# Prepare a DataFrame to store Pearson correlation coefficients with new labels\n",
    "correlation_matrix = pd.DataFrame(index=model_labels, columns=strategy_labels)\n",
    "\n",
    "# Calculate Pearson correlation coefficients\n",
    "for model, model_label in zip(models, model_labels):\n",
    "    for strategy, strategy_label in zip(strategies, strategy_labels):\n",
    "        correlation, _ = pearsonr(flatten_col(model), flatten_col(strategy))\n",
    "        correlation_matrix.loc[model_label, strategy_label] = correlation\n",
    "\n",
    "# Convert the coefficients to float for plotting\n",
    "correlation_matrix = correlation_matrix.astype(float)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(correlation_matrix.T, annot=True, cmap='coolwarm', center=0, fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "plt.xlabel('Training steps', fontsize=14)\n",
    "plt.ylabel('Strategies', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.savefig('Correlation coefficients heatmap.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668fa76-d0cd-4db0-9fcf-fcd6716d355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_col(col):\n",
    "    col_list = df[col].tolist()\n",
    "    return [item for sublist in col_list for item in sublist]\n",
    "\n",
    "# Prepare data for heatmap\n",
    "model_steps = ['rule_model', 'checkpoint-100', 'checkpoint-200', 'checkpoint-300', 'checkpoint-400']\n",
    "model_labels = ['0', '100', '200', '300', '400']\n",
    "strategy_labels = ['NR_preds', 'RR_preds', 'TR_preds', 'BR_preds']\n",
    "strategy_names = ['Model\\nfree', 'Reward\\nreval.', 'Transition\\nreval.', 'Model\\nbased']\n",
    "\n",
    "coefficients = pd.DataFrame(columns=model_labels, index=strategy_names)\n",
    "\n",
    "for model, label in zip(model_steps, model_labels):\n",
    "    targets = flatten_col(model)\n",
    "    feats = [flatten_col(strategy) for strategy in strategy_labels]\n",
    "    feats = [list(row) for row in zip(*feats)]\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(feats, targets)\n",
    "    coefficients[label] = reg.coef_\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(coefficients, annot=True, cmap='coolwarm', center=0, fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "plt.xlabel('Training steps', fontsize=14)\n",
    "plt.ylabel('Strategies', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.savefig('Regression coefficients heatmap.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d202a2-06ee-40fe-8833-6b2649627805",
   "metadata": {},
   "source": [
    "#### 'Retrieval augmented generation'\n",
    "\n",
    "Can the generative network's outputs be conditioned on hippocampal sequences (inspired by retrieval augmented generation), to support some degree of generalisation soon after encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dd80a-2e62-437e-9681-764c0210d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(base_model='rule_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05991785-9d6f-44d0-8469-0b2345cf61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(train_seqs, test_seq, n):\n",
    "    # Extract words from the test_seq up to \"SEQUENCE\"\n",
    "    test_seq_words = set(test_seq.split(\"SEQUENCE:\")[0].split())\n",
    "    start_condition = test_seq.split(\"STOP:\")[0]\n",
    "    train_seqs = [t for t in train_seqs if start_condition in t]\n",
    "\n",
    "    overlap_scores = []\n",
    "\n",
    "    for train_seq in train_seqs:\n",
    "        # Extract words from the train_seq up to \"SEQUENCE\"\n",
    "        train_seq_words = set(train_seq.split(\"SEQUENCE:\")[0].split())\n",
    "        \n",
    "        # Calculate the overlap by finding the intersection of sets\n",
    "        overlap = len(test_seq_words.intersection(train_seq_words))\n",
    "        \n",
    "        # Store the train sequence with its overlap score\n",
    "        overlap_scores.append((train_seq, overlap))\n",
    "\n",
    "    # Sort the list of tuples by overlap score in descending order and select the top n\n",
    "    top_n_seqs = sorted(overlap_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    # Return only the sequences, not their scores\n",
    "    return [seq[0] for seq in top_n_seqs]\n",
    "\n",
    "# Example usage\n",
    "train_seqs = [\n",
    "    \"START: blue animal, STOP: green, REWARD: vehicle, SEQUENCE: blue vehicle (3)\",\n",
    "    \"START: green animal, STOP: red, REWARD: vehicle, SEQUENCE: red vehicle (2)\",\n",
    "    \"START: yellow animal, STOP: red, REWARD: vegetable, SEQUENCE: red vegetable (2)\",\n",
    "    \"START: green animal, STOP: blue, REWARD: vehicle, SEQUENCE: blue vehicle (1)\",\n",
    "    \"START: green animal, STOP: red, REWARD: bicycle, SEQUENCE: red bicycle (2)\"\n",
    "]\n",
    "test_seq = \"START: green animal, STOP: red, REWARD: vehicle, SEQUENCE: red vehicle (2)\"\n",
    "\n",
    "print(get_most_similar(train_seqs, test_seq, n=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7df9c-2a09-4f6b-b660-8d6176b3a48e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def get_seqs_RAG(base_dir='clm_script_0', test_type='reward'):\n",
    "    with open(os.path.join(base_dir, f'test_{test_type}.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        test_seqs = sorted(list(set(seqs)))\n",
    "\n",
    "    with open(os.path.join(base_dir, f'train.txt')) as f:\n",
    "        seqs = f.readlines()\n",
    "        seqs = [s.replace('\\n', '') for s in seqs]\n",
    "        train_seqs = sorted(list(set(seqs)))\n",
    "    \n",
    "    return train_seqs, test_seqs\n",
    "        \n",
    "def test_RAG(base_dir = 'clm_script_0', test_type='transition', n=1):\n",
    "    train_seqs, test_seqs = get_seqs_RAG(base_dir=base_dir, test_type=test_type)\n",
    "    print(len(train_seqs))\n",
    "\n",
    "    result_bools = []\n",
    "    mf_baseline_bools = []\n",
    "    nc_baseline_bools = []\n",
    "\n",
    "    for test in test_seqs:\n",
    "        print(f\"Test sequence: {test}\")\n",
    "        train_examples = get_most_similar(train_seqs, test, n=n)\n",
    "        print(f\"Train example: {train_examples}\")\n",
    "        \n",
    "        continuation = model.continue_input(\"\\n\".join(train_examples) + '\\n' + test[0:test.index('SEQUENCE')] + 'SEQUENCE:',\n",
    "                                            do_sample=False)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', len(train_examples) + 1)\n",
    "        end_ind = find_nth(continuation, 'START', len(train_examples) + 2)\n",
    "\n",
    "        true_a_v_r = get_accept_reject_choice(test)\n",
    "        pred_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == pred_a_v_r:\n",
    "            result_bools.append(1)\n",
    "        else:\n",
    "            result_bools.append(0)\n",
    "\n",
    "        mf_baseline_a_v_r = get_accept_reject_choice(train_examples[0])\n",
    "        if true_a_v_r == mf_baseline_a_v_r:\n",
    "            mf_baseline_bools.append(1)\n",
    "        else:\n",
    "            mf_baseline_bools.append(0)\n",
    "        \n",
    "        continuation = model.continue_input(test[0:test.index('SEQUENCE')] + 'SEQUENCE:',\n",
    "                                            do_sample=False)\n",
    "        start_ind = find_nth(continuation, 'SEQUENCE', 1)\n",
    "        end_ind = find_nth(continuation, 'START', 2)\n",
    "        nc_baseline_a_v_r = get_accept_reject_choice(continuation[start_ind:end_ind])\n",
    "        if true_a_v_r == nc_baseline_a_v_r:\n",
    "            nc_baseline_bools.append(1)\n",
    "        else:\n",
    "            nc_baseline_bools.append(0)\n",
    "            \n",
    "    return result_bools, mf_baseline_bools, nc_baseline_bools\n",
    "\n",
    "all_transition = []\n",
    "all_reward = []\n",
    "all_both = []\n",
    "\n",
    "for i in range(5):\n",
    "    res = test_RAG(base_dir = f'clm_script_{i}', test_type='reward', n=1)\n",
    "    all_reward.append(res)\n",
    "    res = test_RAG(base_dir = f'clm_script_{i}', test_type='transition', n=1)\n",
    "    all_transition.append(res)\n",
    "    res = test_RAG(base_dir = f'clm_script_{i}', test_type='both', n=1)\n",
    "    all_both.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e4df7-68b4-4e15-80fd-59e979bc4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/rag_reward.pkl', 'wb') as f:\n",
    "    pickle.dump(all_reward, f)\n",
    "\n",
    "with open('data/rag_transition.pkl', 'wb') as f:\n",
    "    pickle.dump(all_transition, f)\n",
    "\n",
    "with open('data/rag_both.pkl', 'wb') as f:\n",
    "    pickle.dump(all_both, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97fce09-4d74-44cf-b31f-069224d48db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/rag_reward.pkl', 'rb') as f:\n",
    "    all_reward = pickle.load(f)\n",
    "\n",
    "with open('data/rag_transition.pkl', 'rb') as f:\n",
    "    all_transition = pickle.load(f)\n",
    "\n",
    "with open('data/rag_both.pkl', 'rb') as f:\n",
    "    all_both = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486aaf0-2fb0-4936-9e6e-52f3366db007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "transition_accuracies_rag = [np.mean(t[0]) for t in all_transition]\n",
    "transition_accuracies_mf = [np.mean(t[1]) for t in all_transition]\n",
    "transition_accuracies_nc = [np.mean(t[2]) for t in all_transition]\n",
    "\n",
    "reward_accuracies_rag = [np.mean(r[0]) for r in all_reward]\n",
    "reward_accuracies_mf = [np.mean(r[1]) for r in all_reward]\n",
    "reward_accuracies_nc = [np.mean(r[2]) for r in all_reward]\n",
    "\n",
    "both_accuracies_rag = [np.mean(r[0]) for r in all_both]\n",
    "both_accuracies_mf = [np.mean(r[1]) for r in all_both]\n",
    "both_accuracies_nc = [np.mean(r[2]) for r in all_both]\n",
    "\n",
    "# Calculate means\n",
    "transition_means_rag = np.mean(transition_accuracies_rag)\n",
    "transition_means_mf = np.mean(transition_accuracies_mf)\n",
    "transition_means_nc = np.mean(transition_accuracies_nc)\n",
    "\n",
    "reward_means_rag = np.mean(reward_accuracies_rag)\n",
    "reward_means_mf = np.mean(reward_accuracies_mf)\n",
    "reward_means_nc = np.mean(reward_accuracies_nc)\n",
    "\n",
    "both_means_rag = np.mean(both_accuracies_rag)\n",
    "both_means_mf = np.mean(both_accuracies_mf)\n",
    "both_means_nc = np.mean(both_accuracies_nc)\n",
    "\n",
    "# Calculate SEMs\n",
    "transition_sem_rag = np.std(transition_accuracies_rag) / np.sqrt(len(transition_accuracies_rag))\n",
    "transition_sem_mf = np.std(transition_accuracies_mf) / np.sqrt(len(transition_accuracies_mf))\n",
    "transition_sem_nc = np.std(transition_accuracies_nc) / np.sqrt(len(transition_accuracies_nc))\n",
    "\n",
    "reward_sem_rag = np.std(reward_accuracies_rag) / np.sqrt(len(reward_accuracies_rag))\n",
    "reward_sem_mf = np.std(reward_accuracies_mf) / np.sqrt(len(reward_accuracies_mf))\n",
    "reward_sem_nc = np.std(reward_accuracies_nc) / np.sqrt(len(reward_accuracies_nc))\n",
    "\n",
    "both_sem_rag = np.std(both_accuracies_rag) / np.sqrt(len(both_accuracies_rag))\n",
    "both_sem_mf = np.std(both_accuracies_mf) / np.sqrt(len(both_accuracies_mf))\n",
    "both_sem_nc = np.std(both_accuracies_nc) / np.sqrt(len(both_accuracies_nc))\n",
    "\n",
    "# Data to plot\n",
    "labels = ['NC only', 'HPC only', 'RAG']\n",
    "\n",
    "transition_means = [transition_means_nc, transition_means_mf, transition_means_rag]\n",
    "reward_means = [reward_means_nc, reward_means_mf, reward_means_rag]\n",
    "both_means = [both_means_nc, both_means_mf, both_means_rag]\n",
    "\n",
    "transition_sems = [transition_sem_nc, transition_sem_mf, transition_sem_rag]\n",
    "reward_sems = [reward_sem_nc, reward_sem_mf, reward_sem_rag]\n",
    "both_sems = [both_sem_nc, both_sem_mf, both_sem_rag]\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "bar_width = 0.25  # Adjusted bar width for better visual separation\n",
    "index = np.arange(len(labels))\n",
    "opacity = 0.4\n",
    "\n",
    "# Plot bars with SEM error bars\n",
    "bars1 = ax.bar(index - bar_width, transition_means, bar_width, alpha=opacity, yerr=transition_sems, capsize=2, \n",
    "               label='Transition', color='red')\n",
    "bars2 = ax.bar(index, reward_means, bar_width, alpha=opacity, yerr=reward_sems, capsize=2, label='Reward',\n",
    "              color='blue')\n",
    "bars3 = ax.bar(index + bar_width, both_means, bar_width, alpha=opacity, yerr=both_sems, capsize=2, label='Both',\n",
    "              color='purple')\n",
    "\n",
    "# Finalizing the plot\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(title=\"Revaluation Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('RAG_graph_by_method_plan.png', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
